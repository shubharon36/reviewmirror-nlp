{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f178b9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting river\n",
      "  Downloading river-0.23.0-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from river) (1.26.4)\n",
      "Collecting pandas<3.0.0,>=2.2.3 (from river)\n",
      "  Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scipy<2.0.0,>=1.14.1 (from river)\n",
      "  Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.2.3->river) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.2.3->river) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3.0.0,>=2.2.3->river) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.3->river) (1.16.0)\n",
      "Downloading river-0.23.0-cp312-cp312-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/1.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.3/1.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.3-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.0 MB 2.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.1/11.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.0 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.7/11.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.5/11.0 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.3/11.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.1/11.0 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.9/11.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.0/11.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/38.6 MB 6.3 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.4/38.6 MB 5.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.4/38.6 MB 5.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 4.7/38.6 MB 5.9 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 5.8/38.6 MB 5.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 6.6/38.6 MB 5.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 7.9/38.6 MB 5.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 8.1/38.6 MB 5.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 8.9/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 9.4/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 9.4/38.6 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 4.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 4.3 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 4.3 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 3.4 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 11.5/38.6 MB 3.5 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 12.8/38.6 MB 3.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 14.2/38.6 MB 3.8 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 15.2/38.6 MB 3.8 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 3.8 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 3.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 18.4/38.6 MB 4.0 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 19.1/38.6 MB 4.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 19.9/38.6 MB 4.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 21.0/38.6 MB 4.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 22.0/38.6 MB 4.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 23.1/38.6 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 23.9/38.6 MB 4.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 24.9/38.6 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 26.0/38.6 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 27.3/38.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/38.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 28.8/38.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 30.1/38.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 31.2/38.6 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 32.0/38.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 33.3/38.6 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 34.6/38.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.1/38.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.7/38.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 36.2/38.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 36.7/38.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, pandas, river\n",
      "Successfully installed pandas-2.3.3 river-0.23.0 scipy-1.16.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.16.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e01512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] Created run directory: runs\\enhanced_v1_20251119_001142\n",
      "[load] Reading: C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\n",
      "[rows] 200,000 reviews after cleaning\n",
      "[features] Computed hybrid sentiment + style features\n",
      "[enhanced] Building trajectories with change-point detection...\n",
      "[users] 2,657 users with ≥ 5 reviews\n",
      "[clustering] Performing k-means clustering on drift patterns...\n",
      "[clustering] Silhouette: 0.603, Davies-Bouldin: 0.947\n",
      "\n",
      "[clustering] Cluster summary:\n",
      "         drift_slope                        tv                 flip_rate  \\\n",
      "               mean       std count      mean       std count      mean   \n",
      "cluster                                                                   \n",
      "-1              NaN       NaN     0       NaN       NaN     0       NaN   \n",
      " 0        -0.000720  0.020315   301  3.527278  1.663896   301  0.349218   \n",
      " 1        -0.003085  0.041444  1791  0.666282  0.549278  1791  0.012873   \n",
      " 2        -0.019121  0.099315   233  1.751493  0.794389   233  0.706724   \n",
      " 3         0.284640  0.194291    39  0.713933  0.474161    39  0.190171   \n",
      "\n",
      "                        n_changepoints             \n",
      "              std count           mean  std count  \n",
      "cluster                                            \n",
      "-1            NaN     0            0.0  0.0   293  \n",
      " 0       0.180798   301            0.0  0.0   301  \n",
      " 1       0.058377  1791            0.0  0.0  1791  \n",
      " 2       0.217907   233            0.0  0.0   233  \n",
      " 3       0.301646    39            0.0  0.0    39  \n",
      "[splits] Train: 1859, Val: 398, Test: 400\n",
      "[save] Saved data artifacts\n",
      "[metrics] Saved enhanced metrics\n",
      "[viz] Generating enhanced visualizations...\n",
      "\n",
      "============================================================\n",
      "[DONE] Enhanced run complete!\n",
      "[DONE] Outputs in: runs\\enhanced_v1_20251119_001142\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== KEY FINDINGS ===\n",
      "Users with change-points: 0/400\n",
      "Average change-points per user: 0.00\n",
      "Clustering produced 4 distinct user groups\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "run_enhanced_v1.py\n",
    "Enhanced pipeline for ReviewMirror – Milestone 3\n",
    "Adds: (1) ADWIN change-point detection, (2) User clustering, (3) Optional DistilBERT sentiment\n",
    "\"\"\"\n",
    "\n",
    "import os, gzip, json, re, datetime as dt, warnings\n",
    "from typing import Iterator, Dict, Any, List, Tuple, Optional\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# Sentiment\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    _VADER = SentimentIntensityAnalyzer()\n",
    "except:\n",
    "    _VADER = None\n",
    "\n",
    "# ADWIN for change-point detection\n",
    "try:\n",
    "    from river import drift\n",
    "    _ADWIN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    _ADWIN_AVAILABLE = False\n",
    "    print(\"[WARNING] river not installed. Install with: pip install river\")\n",
    "\n",
    "# Regex helpers\n",
    "_WORD = re.compile(r\"\\b\\w+\\b\")\n",
    "_FIRST_PERSON = re.compile(r\"\\b(i|i'm|i've|my|me)\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 1 — Data Loading (same as baseline)\n",
    "# ==============================================================\n",
    "\n",
    "def _open_text(path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def parse_amz_reviews(path: str, n_limit: Optional[int] = None) -> Iterator[Dict[str, Any]]:\n",
    "    with _open_text(path) as f:\n",
    "        head = f.read(2048)\n",
    "        if not head:\n",
    "            return\n",
    "        is_array = head.lstrip().startswith(\"[\")\n",
    "\n",
    "    if is_array:\n",
    "        with _open_text(path) as f:\n",
    "            data = json.load(f)\n",
    "            for i, obj in enumerate(data):\n",
    "                yield obj\n",
    "                if n_limit is not None and i+1 >= n_limit:\n",
    "                    break\n",
    "    else:\n",
    "        with _open_text(path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "                yield obj\n",
    "                if n_limit is not None and i+1 >= n_limit:\n",
    "                    break\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 2 — Feature Helpers (same as baseline)\n",
    "# ==============================================================\n",
    "\n",
    "def _safe_vader_compound(text: str) -> float:\n",
    "    t = text or \"\"\n",
    "    if _VADER is not None:\n",
    "        try:\n",
    "            return float(_VADER.polarity_scores(t)[\"compound\"])\n",
    "        except:\n",
    "            pass\n",
    "    tl = t.lower()\n",
    "    pos = sum(tl.count(w) for w in [\" good\", \" great\", \" excellent\", \" amazing\", \" love\", \" perfect\", \" happy\"])\n",
    "    neg = sum(tl.count(w) for w in [\" bad\", \" terrible\", \" awful\", \" hate\", \" poor\", \" broken\", \" angry\"])\n",
    "    return 0.0 if (pos + neg) == 0 else (pos - neg) / (pos + neg)\n",
    "\n",
    "def _stars_to_unit(stars: float) -> float:\n",
    "    try:\n",
    "        s = float(stars)\n",
    "        return (s - 3.0) / 2.0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def _month_floor(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=ts.year, month=ts.month, day=1, tz=\"UTC\")\n",
    "\n",
    "def _is_ascii_major(text) -> bool:\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return True\n",
    "    ascii_chars = sum(1 for ch in text if ord(ch) < 128)\n",
    "    return ascii_chars >= 0.9 * len(text)\n",
    "\n",
    "def _to_int_or_nan(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return int(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def _style_feats(txt: str) -> Tuple[float, float, float]:\n",
    "    if not isinstance(txt, str) or not txt:\n",
    "        return (0.0, 0.0, 0.0)\n",
    "    n_chars = max(1, len(txt))\n",
    "    tokens = _WORD.findall(txt)\n",
    "    n_tok = max(1, len(tokens))\n",
    "    exclam = txt.count(\"!\") / n_chars\n",
    "    first_person = len(_FIRST_PERSON.findall(txt)) / n_tok\n",
    "    caps = sum(1 for ch in txt if ch.isupper()) / n_chars\n",
    "    return exclam, first_person, caps\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 3 — Build DataFrame (same as baseline)\n",
    "# ==============================================================\n",
    "\n",
    "def build_reviews_df(path: str, n_limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for rec in parse_amz_reviews(path, n_limit=n_limit):\n",
    "        rows.append({\n",
    "            \"user_id\": rec.get(\"reviewerID\") or rec.get(\"user_id\"),\n",
    "            \"item_id\": rec.get(\"asin\") or rec.get(\"business_id\"),\n",
    "            \"text\": rec.get(\"reviewText\") or rec.get(\"text\"),\n",
    "            \"stars\": rec.get(\"overall\") or rec.get(\"stars\"),\n",
    "            \"ts\": pd.to_datetime(\n",
    "                rec.get(\"unixReviewTime\"), unit=\"s\", utc=True\n",
    "            ) if rec.get(\"unixReviewTime\") is not None else (\n",
    "                pd.to_datetime(rec.get(\"reviewTime\"), utc=True, errors=\"coerce\")\n",
    "                if rec.get(\"reviewTime\") is not None else\n",
    "                pd.to_datetime(rec.get(\"date\"), utc=True, errors=\"coerce\")\n",
    "            ),\n",
    "            \"summary\": rec.get(\"summary\"),\n",
    "            \"helpful_votes\": _to_int_or_nan(rec.get(\"vote\") or rec.get(\"useful\")),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.dropna(subset=[\"user_id\", \"item_id\", \"ts\"]).reset_index(drop=True)\n",
    "    df = df[df[\"text\"].map(_is_ascii_major)]\n",
    "    return df\n",
    "\n",
    "def compute_features(df: pd.DataFrame, alpha: float = 0.7) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"sent_text\"]  = df[\"text\"].map(_safe_vader_compound).astype(float)\n",
    "    df[\"sent_stars\"] = df[\"stars\"].map(_stars_to_unit).astype(float)\n",
    "    df[\"sent_hybrid\"] = alpha * df[\"sent_text\"] + (1.0 - alpha) * df[\"sent_stars\"]\n",
    "\n",
    "    feats = df[\"text\"].map(_style_feats).tolist()\n",
    "    df[[\"style_exclam_rate\",\"style_firstperson_rate\",\"style_caps_rate\"]] = pd.DataFrame(\n",
    "        feats, index=df.index\n",
    "    )\n",
    "    df[\"month\"] = df[\"ts\"].map(_month_floor)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 4 — ENHANCEMENT 1: ADWIN Change-Point Detection\n",
    "# ==============================================================\n",
    "\n",
    "def detect_changepoints_adwin(sentiment_series: np.ndarray, delta: float = 0.002) -> List[int]:\n",
    "    \"\"\"\n",
    "    Detect change-points in sentiment time series using ADWIN.\n",
    "    \n",
    "    Args:\n",
    "        sentiment_series: Array of sentiment values over time\n",
    "        delta: Confidence parameter (lower = more sensitive)\n",
    "    \n",
    "    Returns:\n",
    "        List of indices where change-points detected\n",
    "    \"\"\"\n",
    "    if not _ADWIN_AVAILABLE or len(sentiment_series) < 3:\n",
    "        return []\n",
    "    \n",
    "    adwin = drift.ADWIN(delta=delta)\n",
    "    changepoints = []\n",
    "    \n",
    "    for i, val in enumerate(sentiment_series):\n",
    "        if not np.isnan(val):\n",
    "            adwin.update(val)\n",
    "            if adwin.drift_detected:\n",
    "                changepoints.append(i)\n",
    "    \n",
    "    # Merge close changepoints (within 1 position)\n",
    "    if len(changepoints) > 0:\n",
    "        merged = [changepoints[0]]\n",
    "        for cp in changepoints[1:]:\n",
    "            if cp - merged[-1] > 1:\n",
    "                merged.append(cp)\n",
    "        return merged\n",
    "    return []\n",
    "\n",
    "\n",
    "def compute_changepoint_metrics(changepoints: List[int], total_length: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute summary metrics from changepoint list.\"\"\"\n",
    "    if len(changepoints) == 0:\n",
    "        return {\n",
    "            \"n_changepoints\": 0,\n",
    "            \"time_to_first_cp\": np.nan,\n",
    "            \"mean_dwell_time\": np.nan\n",
    "        }\n",
    "    \n",
    "    time_to_first = changepoints[0] / max(1, total_length - 1)\n",
    "    \n",
    "    if len(changepoints) > 1:\n",
    "        dwells = np.diff(changepoints)\n",
    "        mean_dwell = float(np.mean(dwells))\n",
    "    else:\n",
    "        mean_dwell = np.nan\n",
    "    \n",
    "    return {\n",
    "        \"n_changepoints\": len(changepoints),\n",
    "        \"time_to_first_cp\": time_to_first,\n",
    "        \"mean_dwell_time\": mean_dwell\n",
    "    }\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 5 — Build Trajectories with Change-Points\n",
    "# ==============================================================\n",
    "\n",
    "def _group_apply_include_groups(grouper, func):\n",
    "    try:\n",
    "        return grouper.apply(func, include_groups=False)\n",
    "    except TypeError:\n",
    "        return grouper.apply(func)\n",
    "\n",
    "def build_user_trajectories_enhanced(df: pd.DataFrame, min_reviews: int = 5, \n",
    "                                     adwin_delta: float = 0.002):\n",
    "    \"\"\"Enhanced version with change-point detection.\"\"\"\n",
    "    df = df.sort_values([\"user_id\",\"ts\"]).copy()\n",
    "\n",
    "    # Keep users with >= min_reviews\n",
    "    vc = df[\"user_id\"].value_counts()\n",
    "    keep = set(vc[vc >= min_reviews].index)\n",
    "    df = df[df[\"user_id\"].isin(keep)].copy()\n",
    "\n",
    "    # Within-user normalization\n",
    "    df[\"text_z_user\"] = df.groupby(\"user_id\")[\"sent_text\"].transform(\n",
    "        lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # Monthly aggregates\n",
    "    monthly = (df.groupby([\"user_id\",\"month\"])\n",
    "                 .agg(sent_text_mean=(\"sent_text\",\"mean\"),\n",
    "                      sent_hybrid_mean=(\"sent_hybrid\",\"mean\"),\n",
    "                      stars_mean=(\"stars\",\"mean\"),\n",
    "                      n_reviews=(\"text\",\"count\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    monthly[\"month_ord\"] = monthly[\"month\"].dt.year * 12 + monthly[\"month\"].dt.month\n",
    "\n",
    "    # Baseline drift metrics\n",
    "    def _slope_delta(g: pd.DataFrame) -> Dict[str, float]:\n",
    "        g = g.sort_values(\"month_ord\")\n",
    "        x = g[\"month_ord\"].values.astype(float)\n",
    "        y = g[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(x) < 2 or np.isnan(y).all():\n",
    "            return {\"drift_slope\": np.nan, \"drift_delta\": np.nan}\n",
    "        xc = x - x.mean()\n",
    "        denom = float(np.dot(xc, xc) + 1e-8)\n",
    "        slope = float(np.dot(xc, (y - y.mean())) / denom)\n",
    "        delta = float(y[-1] - y[0])\n",
    "        return {\"drift_slope\": slope, \"drift_delta\": delta}\n",
    "\n",
    "    drift = (_group_apply_include_groups(monthly.groupby(\"user_id\"), _slope_delta)\n",
    "                .apply(pd.Series)\n",
    "                .reset_index())\n",
    "\n",
    "    def _extra_metrics(g: pd.DataFrame) -> Dict[str, float]:\n",
    "        y = g.sort_values(\"month_ord\")[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(y) < 2 or np.isnan(y).all():\n",
    "            return {\"tv\": np.nan, \"flip_rate\": np.nan}\n",
    "        tv = float(np.nansum(np.abs(np.diff(y))))\n",
    "        flips = float(np.nansum(np.sign(y[1:]) != np.sign(y[:-1])))\n",
    "        flip_rate = flips / (len(y) - 1)\n",
    "        return {\"tv\": tv, \"flip_rate\": flip_rate}\n",
    "\n",
    "    extra = (_group_apply_include_groups(monthly.groupby(\"user_id\"), _extra_metrics)\n",
    "                .apply(pd.Series)\n",
    "                .reset_index())\n",
    "\n",
    "    # NEW: Change-point detection per user\n",
    "    def _changepoint_analysis(g: pd.DataFrame) -> Dict[str, Any]:\n",
    "        g = g.sort_values(\"month_ord\")\n",
    "        y = g[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        cps = detect_changepoints_adwin(y, delta=adwin_delta)\n",
    "        metrics = compute_changepoint_metrics(cps, len(y))\n",
    "        metrics[\"changepoints\"] = cps\n",
    "        return metrics\n",
    "    \n",
    "    if _ADWIN_AVAILABLE:\n",
    "        cp_data = (_group_apply_include_groups(monthly.groupby(\"user_id\"), _changepoint_analysis)\n",
    "                      .apply(pd.Series)\n",
    "                      .reset_index())\n",
    "    else:\n",
    "        cp_data = pd.DataFrame({\n",
    "            \"user_id\": drift[\"user_id\"],\n",
    "            \"n_changepoints\": 0,\n",
    "            \"time_to_first_cp\": np.nan,\n",
    "            \"mean_dwell_time\": np.nan,\n",
    "            \"changepoints\": [[] for _ in range(len(drift))]\n",
    "        })\n",
    "\n",
    "    seqs = (monthly.groupby(\"user_id\")\n",
    "                  .agg(months=(\"month\", list),\n",
    "                       sent_hybrid_seq=(\"sent_hybrid_mean\", list),\n",
    "                       stars_seq=(\"stars_mean\", list),\n",
    "                       counts_seq=(\"n_reviews\", list))\n",
    "                  .reset_index())\n",
    "\n",
    "    traj = pd.merge(drift, seqs, on=\"user_id\", how=\"left\")\n",
    "    traj = pd.merge(traj, extra, on=\"user_id\", how=\"left\")\n",
    "    traj = pd.merge(traj, cp_data, on=\"user_id\", how=\"left\")\n",
    "    \n",
    "    return df, monthly, traj\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 6 — ENHANCEMENT 2: User Clustering\n",
    "# ==============================================================\n",
    "\n",
    "def cluster_users_by_drift(traj: pd.DataFrame, n_clusters: int = 4, \n",
    "                           features: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster users by drift patterns using k-means.\n",
    "    \n",
    "    Args:\n",
    "        traj: User trajectories dataframe\n",
    "        n_clusters: Number of clusters\n",
    "        features: List of features to use (default: slope, tv, flip_rate)\n",
    "    \n",
    "    Returns:\n",
    "        traj with added 'cluster' column\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = [\"drift_slope\", \"tv\", \"flip_rate\"]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = traj[features].copy()\n",
    "    valid_mask = X.notna().all(axis=1)\n",
    "    X_valid = X[valid_mask]\n",
    "    \n",
    "    if len(X_valid) < n_clusters:\n",
    "        print(f\"[WARNING] Not enough valid samples for {n_clusters} clusters\")\n",
    "        traj[\"cluster\"] = -1\n",
    "        return traj\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # K-means\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Assign clusters\n",
    "    traj[\"cluster\"] = -1\n",
    "    traj.loc[valid_mask, \"cluster\"] = clusters\n",
    "    \n",
    "    # Compute cluster quality metrics\n",
    "    if len(np.unique(clusters)) > 1:\n",
    "        sil_score = silhouette_score(X_scaled, clusters)\n",
    "        db_score = davies_bouldin_score(X_scaled, clusters)\n",
    "        print(f\"[clustering] Silhouette: {sil_score:.3f}, Davies-Bouldin: {db_score:.3f}\")\n",
    "    \n",
    "    return traj\n",
    "\n",
    "\n",
    "def analyze_clusters(traj: pd.DataFrame, features: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Generate cluster summary statistics.\"\"\"\n",
    "    if features is None:\n",
    "        features = [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\", \"n_changepoints\"]\n",
    "    \n",
    "    cluster_summary = traj.groupby(\"cluster\")[features].agg(['mean', 'std', 'count'])\n",
    "    return cluster_summary\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 7 — Data Splits & Evaluation\n",
    "# ==============================================================\n",
    "\n",
    "def create_data_splits(traj: pd.DataFrame, monthly: pd.DataFrame, \n",
    "                       split_ratio=(0.7, 0.15, 0.15), strategy=\"temporal\"):\n",
    "    if strategy == \"temporal\":\n",
    "        user_first_review = monthly.groupby(\"user_id\")[\"month\"].min()\n",
    "        sorted_users = user_first_review.sort_values().index.tolist()\n",
    "    else:\n",
    "        sorted_users = traj[\"user_id\"].sample(frac=1, random_state=42).tolist()\n",
    "    \n",
    "    n = len(sorted_users)\n",
    "    train_end = int(n * split_ratio[0])\n",
    "    val_end = train_end + int(n * split_ratio[1])\n",
    "    \n",
    "    return {\n",
    "        \"train\": sorted_users[:train_end],\n",
    "        \"val\": sorted_users[train_end:val_end],\n",
    "        \"test\": sorted_users[val_end:]\n",
    "    }\n",
    "\n",
    "def compute_baseline_statistics(traj: pd.DataFrame, splits: Dict):\n",
    "    train_traj = traj[traj[\"user_id\"].isin(splits[\"train\"])]\n",
    "    \n",
    "    baselines = {\n",
    "        \"mean_drift_slope\": float(train_traj[\"drift_slope\"].mean()),\n",
    "        \"mean_drift_delta\": float(train_traj[\"drift_delta\"].mean()),\n",
    "        \"median_drift_slope\": float(train_traj[\"drift_slope\"].median()),\n",
    "        \"median_drift_delta\": float(train_traj[\"drift_delta\"].median()),\n",
    "        \"std_drift_slope\": float(train_traj[\"drift_slope\"].std()),\n",
    "        \"std_drift_delta\": float(train_traj[\"drift_delta\"].std()),\n",
    "    }\n",
    "    \n",
    "    return baselines\n",
    "\n",
    "def evaluate_drift_metrics(traj: pd.DataFrame, splits: Dict, baselines: Dict):\n",
    "    test_traj = traj[traj[\"user_id\"].isin(splits[\"test\"])]\n",
    "    \n",
    "    metrics = {\n",
    "        \"test_size\": int(len(test_traj)),\n",
    "        \"slope_mae\": float(np.abs(test_traj[\"drift_slope\"] - baselines[\"mean_drift_slope\"]).mean()),\n",
    "        \"delta_mae\": float(np.abs(test_traj[\"drift_delta\"] - baselines[\"mean_drift_delta\"]).mean()),\n",
    "        \"high_drift_users\": int((np.abs(test_traj[\"drift_slope\"]) > 0.01).sum()),\n",
    "        \"volatile_users\": int((test_traj[\"tv\"] > test_traj[\"tv\"].quantile(0.75)).sum()),\n",
    "        \"flat_users\": int((np.abs(test_traj[\"drift_slope\"]) < 0.001).sum()),\n",
    "    }\n",
    "    \n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]:\n",
    "        if col in test_traj.columns:\n",
    "            metrics[f\"test_{col}_mean\"] = float(test_traj[col].mean())\n",
    "            metrics[f\"test_{col}_median\"] = float(test_traj[col].median())\n",
    "            metrics[f\"test_{col}_std\"] = float(test_traj[col].std())\n",
    "    \n",
    "    # NEW: Change-point metrics\n",
    "    if \"n_changepoints\" in test_traj.columns:\n",
    "        metrics[\"test_n_changepoints_mean\"] = float(test_traj[\"n_changepoints\"].mean())\n",
    "        metrics[\"test_users_with_cp\"] = int((test_traj[\"n_changepoints\"] > 0).sum())\n",
    "        metrics[\"test_cp_rate\"] = float((test_traj[\"n_changepoints\"] > 0).mean())\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 8 — Enhanced Visualizations\n",
    "# ==============================================================\n",
    "\n",
    "def create_enhanced_plots(traj: pd.DataFrame, monthly: pd.DataFrame, fig_dir: str):\n",
    "    \"\"\"Generate all diagnostic plots including new change-point visualizations.\"\"\"\n",
    "    \n",
    "    # 1. Original drift distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    for ax, col in zip(axes.flat, [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]):\n",
    "        data = traj[col].dropna()\n",
    "        ax.hist(data, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.3f}')\n",
    "        ax.axvline(data.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {data.median():.3f}')\n",
    "        ax.set_title(f\"Distribution of {col}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"drift_distributions.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. NEW: Change-point distribution\n",
    "    if \"n_changepoints\" in traj.columns:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Histogram of number of changepoints\n",
    "        cp_counts = traj[\"n_changepoints\"].value_counts().sort_index()\n",
    "        ax1.bar(cp_counts.index, cp_counts.values, color='coral', edgecolor='black', alpha=0.7)\n",
    "        ax1.set_title(\"Distribution of Change-Points per User\", fontsize=13, fontweight='bold')\n",
    "        ax1.set_xlabel(\"Number of Change-Points\")\n",
    "        ax1.set_ylabel(\"Number of Users\")\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Time to first changepoint\n",
    "        time_to_first = traj[\"time_to_first_cp\"].dropna()\n",
    "        if len(time_to_first) > 0:\n",
    "            ax2.hist(time_to_first, bins=30, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "            ax2.axvline(time_to_first.median(), color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'Median: {time_to_first.median():.2f}')\n",
    "            ax2.set_title(\"Time to First Change-Point (Normalized)\", fontsize=13, fontweight='bold')\n",
    "            ax2.set_xlabel(\"Fraction of Timeline\")\n",
    "            ax2.set_ylabel(\"Number of Users\")\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fig_dir, \"changepoint_analysis.png\"), dpi=220)\n",
    "        plt.close()\n",
    "    \n",
    "    # 3. NEW: Cluster visualization\n",
    "    if \"cluster\" in traj.columns and traj[\"cluster\"].max() > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Scatter: slope vs TV colored by cluster\n",
    "        valid = traj[traj[\"cluster\"] >= 0]\n",
    "        scatter = axes[0].scatter(valid[\"drift_slope\"], valid[\"tv\"], \n",
    "                                 c=valid[\"cluster\"], cmap='tab10', \n",
    "                                 s=30, alpha=0.6, edgecolor='black', linewidth=0.5)\n",
    "        axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        axes[0].set_xlabel(\"Drift Slope\", fontsize=11)\n",
    "        axes[0].set_ylabel(\"Total Variation\", fontsize=11)\n",
    "        axes[0].set_title(\"User Clusters: Slope vs. Volatility\", fontsize=13, fontweight='bold')\n",
    "        plt.colorbar(scatter, ax=axes[0], label='Cluster')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cluster sizes\n",
    "        cluster_sizes = valid[\"cluster\"].value_counts().sort_index()\n",
    "        axes[1].bar(cluster_sizes.index, cluster_sizes.values, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_title(\"Cluster Sizes\", fontsize=13, fontweight='bold')\n",
    "        axes[1].set_xlabel(\"Cluster ID\")\n",
    "        axes[1].set_ylabel(\"Number of Users\")\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fig_dir, \"cluster_analysis.png\"), dpi=220)\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Temporal trends (same as baseline)\n",
    "    monthly_agg = monthly.groupby(\"month\").agg({\n",
    "        \"sent_hybrid_mean\": \"mean\",\n",
    "        \"stars_mean\": \"mean\",\n",
    "        \"n_reviews\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    ax1.plot(monthly_agg[\"month\"], monthly_agg[\"sent_hybrid_mean\"], marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "    ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_title(\"Average Hybrid Sentiment Over Time\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel(\"Mean Sentiment\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.bar(monthly_agg[\"month\"], monthly_agg[\"n_reviews\"], alpha=0.7, color='coral')\n",
    "    ax2.set_title(\"Review Volume Over Time\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Month\")\n",
    "    ax2.set_ylabel(\"Number of Reviews\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"temporal_trends.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Slope vs. volatility\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(traj[\"drift_slope\"], traj[\"tv\"], alpha=0.4, s=20)\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(traj[\"tv\"].median(), color='orange', linestyle='--', alpha=0.5)\n",
    "    plt.xlabel(\"Drift Slope\", fontsize=11)\n",
    "    plt.ylabel(\"Total Variation\", fontsize=11)\n",
    "    plt.title(\"Drift Slope vs. Volatility\", fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"slope_vs_volatility.png\"), dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_user_trajectories_with_cp(traj: pd.DataFrame, monthly: pd.DataFrame, \n",
    "                                   fig_dir: str, n_samples: int = 6, seed: int = 42):\n",
    "    \"\"\"Plot sample trajectories with change-points marked.\"\"\"\n",
    "    sample_users = traj.dropna(subset=[\"sent_hybrid_seq\"]).sample(n_samples, random_state=seed)[\"user_id\"].tolist()\n",
    "    \n",
    "    for i, uid in enumerate(sample_users, start=1):\n",
    "        g = monthly[monthly[\"user_id\"] == uid].sort_values(\"month\")\n",
    "        user_traj = traj[traj[\"user_id\"] == uid].iloc[0]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(np.array(g[\"month\"].values), g[\"sent_hybrid_mean\"], \n",
    "                marker='o', linewidth=2, markersize=6, label='Sentiment')\n",
    "        plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Mark change-points\n",
    "        if \"changepoints\" in user_traj and len(user_traj[\"changepoints\"]) > 0:\n",
    "            cps = user_traj[\"changepoints\"]\n",
    "            months = g[\"month\"].values\n",
    "            for cp_idx in cps:\n",
    "                if cp_idx < len(months):\n",
    "                    plt.axvline(months[cp_idx], color='red', linestyle=':', linewidth=2, alpha=0.7)\n",
    "            plt.scatter([], [], color='red', marker='|', s=100, linewidth=2, label='Change-Point')\n",
    "        \n",
    "        plt.title(f\"User {uid} — Sentiment Trajectory (Slope: {user_traj['drift_slope']:.3f})\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "        plt.xlabel(\"Month\", fontsize=11)\n",
    "        plt.ylabel(\"Hybrid Sentiment\", fontsize=11)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fig_dir, f\"user_traj_{i}.png\"), dpi=220, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 9 — Comparison Framework\n",
    "# ==============================================================\n",
    "\n",
    "def compare_baseline_vs_enhanced(baseline_metrics: Dict, enhanced_metrics: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Generate comparison table.\"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    common_keys = set(baseline_metrics.keys()) & set(enhanced_metrics.keys())\n",
    "    \n",
    "    for key in sorted(common_keys):\n",
    "        comparison.append({\n",
    "            \"Metric\": key,\n",
    "            \"Baseline\": baseline_metrics[key],\n",
    "            \"Enhanced\": enhanced_metrics[key],\n",
    "            \"Change\": enhanced_metrics[key] - baseline_metrics[key] if isinstance(baseline_metrics[key], (int, float)) else \"N/A\"\n",
    "        })\n",
    "    \n",
    "    # Add enhanced-only metrics\n",
    "    enhanced_only = set(enhanced_metrics.keys()) - set(baseline_metrics.keys())\n",
    "    for key in sorted(enhanced_only):\n",
    "        comparison.append({\n",
    "            \"Metric\": key,\n",
    "            \"Baseline\": \"N/A\",\n",
    "            \"Enhanced\": enhanced_metrics[key],\n",
    "            \"Change\": \"NEW\"\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# SECTION 10 — Main Pipeline\n",
    "# ==============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    CONFIG = {\n",
    "        \"dataset_name\": \"Amazon Electronics 5-core\",\n",
    "        \"input_path\": r\"C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\",\n",
    "        \"out_root\": \"runs\",\n",
    "        \"run_name\": \"enhanced_v1\",\n",
    "        \"N_LIMIT\": 200000,\n",
    "        \"MIN_REVIEWS\": 5,\n",
    "        \"ALPHA\": 0.7,\n",
    "        \"seed\": 42,\n",
    "        \"split_strategy\": \"temporal\",\n",
    "        \"split_ratio\": [0.7, 0.15, 0.15],\n",
    "        # Enhancement parameters\n",
    "        \"adwin_delta\": 0.002,  # Change-point sensitivity\n",
    "        \"n_clusters\": 4,       # Number of user clusters\n",
    "        \"clustering_features\": [\"drift_slope\", \"tv\", \"flip_rate\"]\n",
    "    }\n",
    "    np.random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "    # Create run directory\n",
    "    run_id = f'{CONFIG[\"run_name\"]}_{dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    RUN_DIR = os.path.join(CONFIG[\"out_root\"], run_id)\n",
    "    FIG_DIR = os.path.join(RUN_DIR, \"figs\")\n",
    "    DATA_DIR = os.path.join(RUN_DIR, \"data\")\n",
    "    os.makedirs(FIG_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"[run] Created run directory: {RUN_DIR}\")\n",
    "\n",
    "    # Save config\n",
    "    with open(os.path.join(RUN_DIR, \"config.json\"), \"w\") as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "    # Load & preprocess\n",
    "    print(f\"[load] Reading: {CONFIG['input_path']}\")\n",
    "    df = build_reviews_df(CONFIG[\"input_path\"], n_limit=CONFIG[\"N_LIMIT\"])\n",
    "    print(f\"[rows] {len(df):,} reviews after cleaning\")\n",
    "\n",
    "    df = compute_features(df, alpha=CONFIG[\"ALPHA\"])\n",
    "    print(\"[features] Computed hybrid sentiment + style features\")\n",
    "\n",
    "    # Build enhanced trajectories\n",
    "    print(\"[enhanced] Building trajectories with change-point detection...\")\n",
    "    df_clean, monthly, traj = build_user_trajectories_enhanced(\n",
    "        df, \n",
    "        min_reviews=CONFIG[\"MIN_REVIEWS\"],\n",
    "        adwin_delta=CONFIG[\"adwin_delta\"]\n",
    "    )\n",
    "    print(f\"[users] {traj.shape[0]:,} users with ≥ {CONFIG['MIN_REVIEWS']} reviews\")\n",
    "\n",
    "    # Clustering\n",
    "    print(\"[clustering] Performing k-means clustering on drift patterns...\")\n",
    "    traj = cluster_users_by_drift(\n",
    "        traj, \n",
    "        n_clusters=CONFIG[\"n_clusters\"],\n",
    "        features=CONFIG[\"clustering_features\"]\n",
    "    )\n",
    "    \n",
    "    cluster_summary = analyze_clusters(traj, features=CONFIG[\"clustering_features\"] + [\"n_changepoints\"])\n",
    "    print(\"\\n[clustering] Cluster summary:\\n\", cluster_summary)\n",
    "\n",
    "    # Data splits\n",
    "    splits = create_data_splits(traj, monthly, \n",
    "                               split_ratio=CONFIG[\"split_ratio\"],\n",
    "                               strategy=CONFIG[\"split_strategy\"])\n",
    "    print(f\"[splits] Train: {len(splits['train'])}, Val: {len(splits['val'])}, Test: {len(splits['test'])}\")\n",
    "\n",
    "    # Save splits\n",
    "    with open(os.path.join(DATA_DIR, \"splits.json\"), \"w\") as f:\n",
    "        json.dump({k: list(v) for k, v in splits.items()}, f, indent=2)\n",
    "\n",
    "    # Baseline statistics\n",
    "    baselines = compute_baseline_statistics(traj, splits)\n",
    "    with open(os.path.join(RUN_DIR, \"baselines.json\"), \"w\") as f:\n",
    "        json.dump(baselines, f, indent=2)\n",
    "\n",
    "    # Save data\n",
    "    df_clean.to_parquet(os.path.join(DATA_DIR, \"reviews.parquet\"))\n",
    "    monthly.to_parquet(os.path.join(DATA_DIR, \"reviews_monthly.parquet\"))\n",
    "    traj.to_parquet(os.path.join(DATA_DIR, \"user_trajectories.parquet\"))\n",
    "    print(\"[save] Saved data artifacts\")\n",
    "\n",
    "    # Compute metrics\n",
    "    enhanced_metrics = {\n",
    "        \"n_rows\": int(len(df_clean)),\n",
    "        \"n_users\": int(traj[\"user_id\"].nunique()),\n",
    "        \"n_items\": int(df_clean[\"item_id\"].nunique()),\n",
    "    }\n",
    "    \n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\", \"n_changepoints\"]:\n",
    "        if col in traj.columns:\n",
    "            enhanced_metrics[f\"{col}_mean\"] = float(traj[col].mean())\n",
    "            enhanced_metrics[f\"{col}_std\"] = float(traj[col].std())\n",
    "            enhanced_metrics[f\"{col}_min\"] = float(traj[col].min())\n",
    "            enhanced_metrics[f\"{col}_max\"] = float(traj[col].max())\n",
    "\n",
    "    eval_metrics = evaluate_drift_metrics(traj, splits, baselines)\n",
    "    enhanced_metrics.update(eval_metrics)\n",
    "    \n",
    "    # Clustering metrics\n",
    "    if \"cluster\" in traj.columns:\n",
    "        enhanced_metrics[\"n_clusters_used\"] = int(traj[\"cluster\"].nunique() - (1 if -1 in traj[\"cluster\"].values else 0))\n",
    "        for cid in range(CONFIG[\"n_clusters\"]):\n",
    "            cluster_size = int((traj[\"cluster\"] == cid).sum())\n",
    "            enhanced_metrics[f\"cluster_{cid}_size\"] = cluster_size\n",
    "\n",
    "    with open(os.path.join(RUN_DIR, \"metrics_enhanced.json\"), \"w\") as f:\n",
    "        json.dump(enhanced_metrics, f, indent=2)\n",
    "    print(\"[metrics] Saved enhanced metrics\")\n",
    "\n",
    "    # Visualizations\n",
    "    print(\"[viz] Generating enhanced visualizations...\")\n",
    "    create_enhanced_plots(traj, monthly, FIG_DIR)\n",
    "    plot_user_trajectories_with_cp(traj, monthly, FIG_DIR, n_samples=6, seed=CONFIG[\"seed\"])\n",
    "\n",
    "    # Save cluster summary\n",
    "    cluster_summary.to_csv(os.path.join(RUN_DIR, \"cluster_summary.csv\"))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[DONE] Enhanced run complete!\")\n",
    "    print(f\"[DONE] Outputs in: {RUN_DIR}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Print key findings\n",
    "    print(\"\\n=== KEY FINDINGS ===\")\n",
    "    print(f\"Users with change-points: {enhanced_metrics.get('test_users_with_cp', 'N/A')}/{enhanced_metrics['test_size']}\")\n",
    "    print(f\"Average change-points per user: {enhanced_metrics.get('test_n_changepoints_mean', 'N/A'):.2f}\")\n",
    "    print(f\"Clustering produced {enhanced_metrics.get('n_clusters_used', 'N/A')} distinct user groups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
