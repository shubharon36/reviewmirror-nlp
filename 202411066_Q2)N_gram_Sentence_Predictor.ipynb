{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Shubham Kukadiya - 202411066\n",
        "\n",
        "Q2) Build an n-gram-based neural language model to predict the next word in a sentence.\n",
        "\n",
        "1. Tokenize a small text dataset and generate n-gram (n=3 or 4) context-target pairs.\n",
        "2. Use embedding layer + MLP with one hidden layer (ReLU activation).\n",
        "3. Train the model with cross-entropy loss and SGD optimizer.\n",
        "4. Track and print loss after each epoch to verify backpropagation and gradient descent are working.\n",
        "5. Evaluate the model on a few custom test sentences."
      ],
      "metadata": {
        "id": "PlhF6vZhSUWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "TXT_PATH = \"/content/shakespeare.txt\"\n",
        "N = 4\n",
        "EMB_DIM = 128\n",
        "HIDDEN = 256\n",
        "EPOCHS = 8\n",
        "BATCH_SIZE = 256\n",
        "LR = 0.5\n",
        "MIN_FREQ = 1\n",
        "TOPK = 5\n",
        "SEED = 1337\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CSEv2uVNnNB",
        "outputId": "3d2faa16-31e8-4095-c47d-af42efde3e92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dcb73fe0c10>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "WORD_RE = re.compile(r\"[A-Za-z']+|[.,;:?!\\-()\\[\\]\\\"“”’]\")\n",
        "\n",
        "def tokenize_line(line: str) -> List[str]:\n",
        "\n",
        "    toks = WORD_RE.findall(line.lower())\n",
        "\n",
        "    cleaned = [t.replace(\"’\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"') for t in toks]\n",
        "    return [t for t in cleaned if t.strip()]\n",
        "\n",
        "# Data prep: vocab + n-grams\n",
        "SPECIALS = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "PAD, UNK, BOS, EOS = range(4)\n",
        "\n",
        "def build_vocab(lines: List[str], min_freq: int = 1):\n",
        "    tokens = []\n",
        "    for line in lines:\n",
        "        tokens.extend(tokenize_line(line))\n",
        "    freqs = Counter(tokens)\n",
        "    itos = SPECIALS + [t for t, c in freqs.items() if c >= min_freq and t not in SPECIALS]\n",
        "    stoi = {t: i for i, t in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "def line_to_ids(line: str, stoi: dict) -> List[int]:\n",
        "    return [stoi.get(tok, UNK) for tok in tokenize_line(line)]\n",
        "\n",
        "def make_ngrams(lines: List[str], stoi: dict, n: int) -> Tuple[List[List[int]], List[int]]:\n",
        "\n",
        "    contexts, targets = [], []\n",
        "    for line in lines:\n",
        "        ids = line_to_ids(line, stoi)\n",
        "        ids_ext = ids + [EOS]\n",
        "        ctx = [BOS] * (n - 1) + ids_ext\n",
        "        for i in range(n - 1, len(ctx)):\n",
        "            context = ctx[i-(n-1):i]\n",
        "            target = ctx[i]\n",
        "            contexts.append(context)\n",
        "            targets.append(target)\n",
        "    return contexts, targets\n",
        "\n"
      ],
      "metadata": {
        "id": "N3xtJZ0XNy0x"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset\n",
        "class NGramDataset(Dataset):\n",
        "    def __init__(self, contexts: List[List[int]], targets: List[int]):\n",
        "        self.x = torch.tensor(contexts, dtype=torch.long)\n",
        "        self.y = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "tjfnqGx2N1_C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class NGramMLP(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_dim: int, hidden: int, n: int):\n",
        "        super().__init__()\n",
        "        self.n_minus_1 = n - 1\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(emb_dim * self.n_minus_1, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden, vocab_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, context_ids: torch.Tensor):\n",
        "        # context_ids: (B, n-1)\n",
        "        emb = self.emb(context_ids)             # (B, n-1, emb)\n",
        "        flat = emb.reshape(emb.size(0), -1)     # (B, (n-1)*emb)\n",
        "        logits = self.ff(flat)                  # (B, V)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "eOwV6s2QN4i2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zOI7F84cMKtR"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Training / Eval\n",
        "def load_lines(path: str) -> List[str]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read().splitlines()\n",
        "\n",
        "def split_train_valid(ctx, tgt, valid_frac=0.05):\n",
        "    idx = list(range(len(tgt)))\n",
        "    random.shuffle(idx)\n",
        "    cut = int(len(idx) * (1 - valid_frac))\n",
        "    tr_idx, va_idx = idx[:cut], idx[cut:]\n",
        "    tr_x = [ctx[i] for i in tr_idx]\n",
        "    tr_y = [tgt[i] for i in tr_idx]\n",
        "    va_x = [ctx[i] for i in va_idx]\n",
        "    va_y = [tgt[i] for i in va_idx]\n",
        "    return (tr_x, tr_y), (va_x, va_y)\n",
        "\n",
        "def epoch_run(model, loader, criterion, optimizer=None):\n",
        "    is_train = optimizer is not None\n",
        "    total, total_loss = 0, 0.0\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        if is_train:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total += yb.numel()\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "    return total_loss / max(total, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_next_words(model, prompt: str, stoi: dict, itos: List[str], n: int, k: int = 5):\n",
        "    toks = tokenize_line(prompt)\n",
        "    ids = [stoi.get(t, UNK) for t in toks][- (n - 1):]  #  last n-1 tokens\n",
        "    ids = [BOS] * max(0, n - 1 - len(ids)) + ids\n",
        "    ctx = torch.tensor([ids], dtype=torch.long, device=DEVICE)  # (1, n-1)\n",
        "    logits = model(ctx)          # (1, V)\n",
        "    probs = torch.softmax(logits[0], dim=-1)\n",
        "    topk = torch.topk(probs, k=k)\n",
        "    words = [itos[i] for i in topk.indices.tolist()]\n",
        "    return list(zip(words, [float(p) for p in topk.values.tolist()]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(f\"The device is: {DEVICE}\")\n",
        "    lines = load_lines(TXT_PATH)\n",
        "    print(f\"Number of total lines={len(lines)} from {TXT_PATH}\")\n",
        "\n",
        "    # Build vocab\n",
        "    stoi, itos = build_vocab(lines, min_freq=MIN_FREQ)\n",
        "    vocab_size = len(itos)\n",
        "    print(f\"Vocab size={vocab_size}\")\n",
        "\n",
        "    # Build n-gram pairs\n",
        "    contexts, targets = make_ngrams(lines, stoi, n=N)\n",
        "    print(f\"Pairs in total={len(targets)} (n={N} → context size={N-1})\")\n",
        "\n",
        "    # Train/valid split\n",
        "    (tr_x, tr_y), (va_x, va_y) = split_train_valid(contexts, targets, valid_frac=0.05)\n",
        "\n",
        "    train_ds = NGramDataset(tr_x, tr_y)\n",
        "    valid_ds = NGramDataset(va_x, va_y)\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "    # Model\n",
        "    model = NGramMLP(vocab_size, EMB_DIM, HIDDEN, N).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        tr_loss = epoch_run(model, train_loader, criterion, optimizer)\n",
        "        va_loss = epoch_run(model, valid_loader, criterion, optimizer=None)\n",
        "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | valid_loss={va_loss:.4f}\")\n",
        "\n",
        "    # custom prompts\n",
        "    prompts = [\n",
        "        \"shall i compare\",\n",
        "        \"in fair love\",\n",
        "        \"when i do\",\n",
        "        \"my love is\",\n",
        "        \"time doth\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTop-k next-word predictions\")\n",
        "    for s in prompts:\n",
        "        preds = predict_next_words(model, s, stoi, itos, n=N, k=TOPK)\n",
        "        pretty = \", \".join([f\"{w} ({p:.3f})\" for w, p in preds])\n",
        "        print(f\"  '{s}  →'  {pretty}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBBeZSXJN_IK",
        "outputId": "44dcc5ac-4660-438f-99b5-8e5e4f8d6bef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device is: cuda\n",
            "Number of total lines=124185 from /content/shakespeare.txt\n",
            "Vocab size=26980\n",
            "Pairs in total=1241049 (n=4 → context size=3)\n",
            "Epoch 01/8 | train_loss=5.4547 | valid_loss=5.2199\n",
            "Epoch 02/8 | train_loss=5.0600 | valid_loss=5.1337\n",
            "Epoch 03/8 | train_loss=4.9366 | valid_loss=5.1156\n",
            "Epoch 04/8 | train_loss=4.8513 | valid_loss=5.0602\n",
            "Epoch 05/8 | train_loss=4.7835 | valid_loss=5.0960\n",
            "Epoch 06/8 | train_loss=4.7246 | valid_loss=5.0710\n",
            "Epoch 07/8 | train_loss=4.6715 | valid_loss=5.0891\n",
            "Epoch 08/8 | train_loss=4.6231 | valid_loss=5.0996\n",
            "\n",
            "Top-k next-word predictions\n",
            "  'shall i compare  →'  , (0.104), <eos> (0.070), ; (0.035), to (0.028), . (0.022)\n",
            "  'in fair love  →'  , (0.378), . (0.112), <eos> (0.055), ; (0.028), - (0.020)\n",
            "  'when i do  →'  not (0.075), see (0.052), love (0.044), believe (0.029), fear (0.027)\n",
            "  'my love is  →'  dead (0.062), not (0.045), better (0.031), , (0.026), too (0.022)\n",
            "  'time doth  →'  not (0.130), the (0.064), in (0.018), servant (0.016), , (0.013)\n"
          ]
        }
      ]
    }
  ]
}