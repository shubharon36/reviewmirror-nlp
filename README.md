# ReviewMirror â€” Tracking Opinion Change in Amazon Reviews

> **Do people's opinions change over time? We found outâ€”and it's interesting!**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Status](https://img.shields.io/badge/Status-Complete-success.svg)]()

---

## ğŸ¯ What We Did

Most sentiment analysis treats each review independently. We asked: **What if we track the SAME user over time?** Do they get happier? More critical? Unstable?

We analyzed **17,706 Amazon Electronics reviews** from **2,657 users** (1996â€“2018) and discovered **4 distinct types of reviewers**.

---

## ğŸ“Š The 4 Types of Reviewers We Found

| Type | % of Users | What They Do | Example |
|------|------------|--------------|---------|
| **ğŸ˜Š Stable Majority** | 67% | Consistent opinions, barely change | Always happy with purchases |
| **ğŸ¢ Volatile Critics** | 11% | Wild mood swings, unpredictable | Love one product, hate the next |
| **â†”ï¸ Flip-Floppers** | 9% | Constantly switch positive â†” negative | Can't make up their mind |
| **ğŸ“ˆ Improvers** | 1% | Get MUCH happier over time | Started grumpy, ended satisfied |

**Most Interesting Finding:** The "Improvers" (only 39 people!) showed **750Ã— stronger** positive drift than averageâ€”genuine satisfaction improvement over years!

---

## ğŸ”¢ Our Numbers (The Results)

### Main Achievement
- **10.27% better predictions** than the naive approach
- Discovered user groups are **statistically real** (p < 0.0000000001)
- Our method: predict using user's group vs. treating everyone the same

### Comparing Two Approaches

We tested **two ways** to group similar users:

| Method | What It Does | Prediction Accuracy | Cluster Balance | Cluster Tightness |
|--------|--------------|---------------------|-----------------|-------------------|
| **K-means** | Groups by behavior patterns | âœ… **Better** (MAE: 0.0196) | âŒ Unbalanced (67% in 1 group) | âš ï¸ Looser |
| **Graph Neural Network** | Groups by who reviews similar products | âŒ Worse (MAE: 0.0220) | âœ… Balanced (64% max) | âœ… **58% tighter!** |

**Takeaway:** Both methods find *different* user groupsâ€”K-means is better for predicting drift, GNN is better for recommendation systems.

---

## ğŸ§ª How We Did It (Simple Version)
```
Step 1: Get Reviews
   â†“
Step 2: Calculate Sentiment (text + star rating)
   â†“
Step 3: Group by Month (per user)
   â†“
Step 4: Measure Change Over Time
   - Slope: Are they trending up or down?
   - Volatility: How bumpy is the ride?
   - Flips: Do they change their mind often?
   â†“
Step 5: Find Similar Users (clustering)
   â†“
Result: 4 Behavioral Types!
```

### The Formula We Used
```
Sentiment = 70% Ã— (text mood) + 30% Ã— (star rating)
```
Why? Text captures nuance ("good but..."), stars capture overall judgment.


## ğŸš€ Quick Start (3 Steps)

### 1. Install Stuff
```bash
git clone https://github.com/shubharon36/reviewmirror-nlp.git
cd reviewmirror-nlp
pip install pandas numpy matplotlib scikit-learn vaderSentiment pyarrow
```

### 2. Get Data
Download: [Amazon Electronics 5-core](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/)

### 3. Run Notebooks
```python
# Open 00_prep_and_baseline.ipynb
# Change this line to your data location:
CONFIG["input_path"] = r"C:\your\path\to\Electronics_5.json"

# Run all cells (Shift+Enter)
# Takes ~5 minutes
```

**You'll get:**
- ğŸ“Š Charts showing the 4 user types
- ğŸ“ˆ Individual user trajectories
- ğŸ“‰ Statistics proving it works

---

## ğŸ–¼ï¸ Cool Visualizations We Made

### User Types (Box Plot)
![Types](runs/enhanced_v1_20251119_001142/figs/cluster_boxplots.png)

*See how different each group is!*

### Individual Journeys
<table>
  <tr>
    <td><img src="figs/user_traj_1.png" width="400"/></td>
    <td><img src="figs/user_traj_3.png" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><em>Volatile user: sentiment all over the place</em></td>
    <td align="center"><em>Improver: steady upward trend</em></td>
  </tr>
</table>

### K-means vs GNN
![Comparison](runs/enhanced_v1_20251119_001142/figs/cluster_comparison_distribution.png)

*Both find different patternsâ€”both useful!*

---

## ğŸ’¡ Why This Matters

### For Businesses
- **Target "Improvers"** for loyalty programs (they're getting happier!)
- **Watch "Flip-Floppers"** carefully (at-risk customers)
- **Give "Volatile Critics"** better recommendations (they're unpredictable)

### For Research
- Proves user opinions **DO change over time** (not static!)
- Simple methods work well (don't need fancy AI for everything)
- Graph structure reveals complementary patterns

---

## ğŸ”§ Technical Details 

**Dataset:** Amazon Electronics 5-core (2,657 users, 17,706 reviews)  
**Split:** 70% train / 15% validation / 15% test (temporal, not random)  
**Metrics:** MAE, RMSE, Silhouette (0.603), ARI (0.29)  
**Significance:** ANOVA F=87.4, p<10â»âµâ°  
**Baseline:** VADER + stars â†’ k-means(k=4) on [slope, TV, flip_rate]  
**Enhancement:** 2-layer GCN on user-item bipartite graph  

---

## ğŸš§ What's Next?

- [ ] Try on Yelp data (restaurants vs. electronics?)
- [ ] Detect WHEN opinions change (not just HOW MUCH)
- [ ] Use better sentiment models (BERT instead of VADER)
- [ ] Predict future drift from early reviews

---



## ğŸ“š Learn More

- **Dataset Source:** [UCSD Amazon Review Data](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/)
- **VADER Sentiment:** [GitHub](https://github.com/cjhutto/vaderSentiment)
- **Full Paper:** See `NLP_Project_Report.pdf`

---

<div align="center">

</div>
