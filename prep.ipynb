{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a60f72a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pandas numpy pyarrow vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df06a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 1 — Imports & Globals\n",
    "\n",
    "import os, gzip, json, math, re\n",
    "from typing import Iterator, Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sentiment (VADER)\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    _VADER = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    _VADER = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_WORD = re.compile(r\"\\b\\w+\\b\")\n",
    "_FIRST_PERSON = re.compile(r\"\\b(i|i'm|i’ve|i've|my|me)\\b\", re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 2 — Robust Reader for Amazon/Yelp formats\n",
    "\n",
    "def _open_text(path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def parse_amz_reviews(path: str, n_limit: Optional[int]=None) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Yields dict reviews from:\n",
    "      - JSON Lines (usual Amazon format): one JSON object per line\n",
    "      - OR a single large JSON array of objects\n",
    "    \"\"\"\n",
    "    with _open_text(path) as f:\n",
    "        head = f.read(2048)\n",
    "        if not head:\n",
    "            return\n",
    "        is_array = head.lstrip().startswith(\"[\")\n",
    "\n",
    "    if is_array:\n",
    "        with _open_text(path) as f:\n",
    "            data = json.load(f)\n",
    "            for i, obj in enumerate(data):\n",
    "                yield obj\n",
    "                if n_limit is not None and i+1 >= n_limit:\n",
    "                    break\n",
    "    else:\n",
    "        with _open_text(path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                yield obj\n",
    "                if n_limit is not None and i+1 >= n_limit:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 3 — Feature Helpers (VADER, mapping, style, utils)\n",
    "\n",
    "def _safe_vader_compound(text: str) -> float:\n",
    "    \"\"\"\n",
    "    VADER compound in [-1,1]. Uses a global analyzer if available, else a simple fallback.\n",
    "    \"\"\"\n",
    "    t = text or \"\"\n",
    "    if _VADER is not None:\n",
    "        try:\n",
    "            return float(_VADER.polarity_scores(t)[\"compound\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    tl = t.lower()\n",
    "    pos = sum(tl.count(w) for w in [\" good\", \" great\", \" excellent\", \" amazing\", \" love\", \" perfect\", \" happy\"])\n",
    "    neg = sum(tl.count(w) for w in [\" bad\", \" terrible\", \" awful\", \" hate\", \" poor\", \" broken\", \" angry\"])\n",
    "    return 0.0 if (pos + neg) == 0 else (pos - neg) / (pos + neg)\n",
    "\n",
    "def _stars_to_unit(stars: float) -> float:\n",
    "    \"\"\"Map 1..5 -> [-1, 1]\"\"\"\n",
    "    try:\n",
    "        s = float(stars)\n",
    "        return (s - 3.0) / 2.0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _month_floor(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=ts.year, month=ts.month, day=1, tz=\"UTC\")\n",
    "\n",
    "def _is_ascii_major(text) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    if not text:\n",
    "        return True\n",
    "    ascii_chars = sum(1 for ch in text if ord(ch) < 128)\n",
    "    return ascii_chars >= 0.9 * len(text)\n",
    "\n",
    "def _to_int_or_nan(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _style_feats(txt: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"(exclam_per_char, first_person_per_token, caps_per_char)\"\"\"\n",
    "    if not isinstance(txt, str) or not txt:\n",
    "        return (0.0, 0.0, 0.0)\n",
    "    n_chars = max(1, len(txt))\n",
    "    tokens = _WORD.findall(txt)\n",
    "    n_tok = max(1, len(tokens))\n",
    "    exclam = txt.count(\"!\") / n_chars\n",
    "    first_person = len(_FIRST_PERSON.findall(txt)) / n_tok\n",
    "    caps = sum(1 for ch in txt if ch.isupper()) / n_chars\n",
    "    return exclam, first_person, caps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 4 — Build Row-Level DataFrame\n",
    "\n",
    "def build_reviews_df(path: str, n_limit: Optional[int]=None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for rec in parse_amz_reviews(path, n_limit=n_limit):\n",
    "        rows.append({\n",
    "            \"user_id\": rec.get(\"reviewerID\") or rec.get(\"user_id\"),\n",
    "            \"item_id\": rec.get(\"asin\") or rec.get(\"business_id\"),\n",
    "            \"text\": rec.get(\"reviewText\") or rec.get(\"text\"),\n",
    "            \"stars\": rec.get(\"overall\") or rec.get(\"stars\"),\n",
    "            \"ts\": pd.to_datetime(\n",
    "                rec.get(\"unixReviewTime\"), unit=\"s\", utc=True\n",
    "            ) if rec.get(\"unixReviewTime\") is not None else (\n",
    "                pd.to_datetime(rec.get(\"reviewTime\"), utc=True, errors=\"coerce\")\n",
    "                if rec.get(\"reviewTime\") is not None else\n",
    "                pd.to_datetime(rec.get(\"date\"), utc=True, errors=\"coerce\")\n",
    "            ),\n",
    "            \"summary\": rec.get(\"summary\"),\n",
    "            \"helpful_votes\": _to_int_or_nan(rec.get(\"vote\") or rec.get(\"useful\")),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = df.dropna(subset=[\"user_id\", \"item_id\", \"ts\"]).reset_index(drop=True)\n",
    "    df = df[df[\"text\"].map(_is_ascii_major)]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8420889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 5 — Compute Features (sentiment, style, month)\n",
    "\n",
    "def compute_features(df: pd.DataFrame, alpha: float=0.7) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"sent_text\"]  = df[\"text\"].map(_safe_vader_compound).astype(float)\n",
    "    df[\"sent_stars\"] = df[\"stars\"].map(_stars_to_unit).astype(float)\n",
    "    df[\"sent_hybrid\"] = alpha*df[\"sent_text\"] + (1.0-alpha)*df[\"sent_stars\"]\n",
    "\n",
    "    feats = df[\"text\"].map(_style_feats).tolist()\n",
    "    df[[\"style_exclam_rate\",\"style_firstperson_rate\",\"style_caps_rate\"]] = pd.DataFrame(\n",
    "        feats, index=df.index\n",
    "    )\n",
    "    df[\"month\"] = df[\"ts\"].map(_month_floor)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a49f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 6 — Aggregate to Per-User Trajectories + Drift Stats\n",
    "\n",
    "def build_user_trajectories(df: pd.DataFrame, min_reviews: int=5):\n",
    "    df = df.sort_values([\"user_id\",\"ts\"]).copy()\n",
    "\n",
    "    # keep users with >= min_reviews\n",
    "    vc = df[\"user_id\"].value_counts()\n",
    "    keep = set(vc[vc >= min_reviews].index)\n",
    "    df = df[df[\"user_id\"].isin(keep)].copy()\n",
    "\n",
    "    # optional within-user normalization for text sentiment\n",
    "    df[\"text_z_user\"] = df.groupby(\"user_id\")[\"sent_text\"].transform(\n",
    "        lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # monthly aggregates\n",
    "    monthly = (df.groupby([\"user_id\",\"month\"])\n",
    "                 .agg(sent_text_mean=(\"sent_text\",\"mean\"),\n",
    "                      sent_hybrid_mean=(\"sent_hybrid\",\"mean\"),\n",
    "                      stars_mean=(\"stars\",\"mean\"),\n",
    "                      n_reviews=(\"text\",\"count\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    # Use TRUE time for slope: month ordinal (YYYY*12 + MM)\n",
    "    monthly[\"month_ord\"] = monthly[\"month\"].dt.year * 12 + monthly[\"month\"].dt.month\n",
    "\n",
    "    # slope + delta per user (on true time)\n",
    "    def _slope_delta(g: pd.DataFrame) -> Dict[str,float]:\n",
    "        g = g.sort_values(\"month_ord\")\n",
    "        x = g[\"month_ord\"].values.astype(float)\n",
    "        y = g[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(x) < 2 or np.isnan(y).all():\n",
    "            return {\"drift_slope\": np.nan, \"drift_delta\": np.nan}\n",
    "        xc = x - x.mean()\n",
    "        denom = float(np.dot(xc, xc) + 1e-8)\n",
    "        slope = float(np.dot(xc, (y - y.mean())) / denom)\n",
    "        delta = float(y[-1] - y[0])\n",
    "        return {\"drift_slope\": slope, \"drift_delta\": delta}\n",
    "\n",
    "    drift = (monthly.groupby(\"user_id\")\n",
    "                  .apply(_slope_delta)\n",
    "                  .apply(pd.Series)\n",
    "                  .reset_index())\n",
    "\n",
    "    #  total variation, sign-flip rate\n",
    "    def _extra_metrics(g: pd.DataFrame) -> Dict[str, float]:\n",
    "        y = g.sort_values(\"month_ord\")[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(y) < 2 or np.isnan(y).all():\n",
    "            return {\"tv\": np.nan, \"flip_rate\": np.nan}\n",
    "        tv = float(np.nansum(np.abs(np.diff(y))))\n",
    "        flips = float(np.nansum(np.sign(y[1:]) != np.sign(y[:-1])))\n",
    "        flip_rate = flips / (len(y) - 1)\n",
    "        return {\"tv\": tv, \"flip_rate\": flip_rate}\n",
    "\n",
    "    extra = (monthly.groupby(\"user_id\")\n",
    "                   .apply(_extra_metrics)\n",
    "                   .apply(pd.Series)\n",
    "                   .reset_index())\n",
    "\n",
    "    seqs = (monthly.groupby(\"user_id\")\n",
    "                  .agg(months=(\"month\", list),\n",
    "                       sent_hybrid_seq=(\"sent_hybrid_mean\", list),\n",
    "                       stars_seq=(\"stars_mean\", list),\n",
    "                       counts_seq=(\"n_reviews\", list))\n",
    "                  .reset_index())\n",
    "\n",
    "    traj = pd.merge(drift, seqs, on=\"user_id\", how=\"left\")\n",
    "    traj = pd.merge(traj, extra, on=\"user_id\", how=\"left\")\n",
    "    return df, monthly, traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fba0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sanity] VADER available: True\n",
      "[sanity] 'I love it' → 0.6369\n",
      "[sanity] 'I hate it' → -0.5719\n",
      "[load] reading: C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\n",
      "[rows] 200,000 reviews after basic cleaning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m df \u001b[38;5;241m=\u001b[39m build_reviews_df(INPUT, n_limit\u001b[38;5;241m=\u001b[39mN_LIMIT)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[rows] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m reviews after basic cleaning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m compute_features(df, alpha\u001b[38;5;241m=\u001b[39mALPHA)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[features] computed hybrid sentiment + style proxies + month bins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m df_clean, monthly, traj \u001b[38;5;241m=\u001b[39m build_user_trajectories(df, min_reviews\u001b[38;5;241m=\u001b[39mMIN_REVIEWS)\n",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m, in \u001b[0;36mcompute_features\u001b[1;34m(df, alpha)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_features\u001b[39m(df: pd\u001b[38;5;241m.\u001b[39mDataFrame, alpha: \u001b[38;5;28mfloat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m      6\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m----> 7\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(_safe_vader_compound)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      8\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_stars\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstars\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(_stars_to_unit)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[0;32m      9\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_hybrid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m alpha\u001b[38;5;241m*\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\u001b[38;5;241m*\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_stars\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4700\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[0;32m   4621\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4622\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   4623\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4624\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4626\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4627\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4698\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4699\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4700\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_values(arg, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m   4701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4702\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4703\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m, in \u001b[0;36m_safe_vader_compound\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VADER \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(_VADER\u001b[38;5;241m.\u001b[39mpolarity_scores(t)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vaderSentiment\\vaderSentiment.py:269\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    266\u001b[0m         sentiments\u001b[38;5;241m.\u001b[39mappend(valence)\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m     sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentiment_valence(valence, sentitext, item, i, sentiments)\n\u001b[0;32m    271\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_but_check(words_and_emoticons, sentiments)\n\u001b[0;32m    273\u001b[0m valence_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_valence(sentiments, text)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vaderSentiment\\vaderSentiment.py:312\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.sentiment_valence\u001b[1;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[0;32m    310\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[0;32m    311\u001b[0m valence \u001b[38;5;241m=\u001b[39m valence \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 312\u001b[0m valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_negation_check(valence, words_and_emoticons, start_i, i)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    314\u001b[0m     valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_special_idioms_check(valence, words_and_emoticons, i)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\vaderSentiment\\vaderSentiment.py:402\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer._negation_check\u001b[1;34m(valence, words_and_emoticons, start_i, i)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_negation_check\u001b[39m(valence, words_and_emoticons, start_i, i):\n\u001b[1;32m--> 402\u001b[0m     words_and_emoticons_lower \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(w)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words_and_emoticons]\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m negated([words_and_emoticons_lower[i \u001b[38;5;241m-\u001b[39m (start_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]]):  \u001b[38;5;66;03m# 1 word preceding lexicon word (w/o stopwords)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SECTION 7 — Main: Config, Run, Save, Quick Plots\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT = r\"C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\"   \n",
    "    OUTDIR = \"data\"\n",
    "    MIN_REVIEWS = 5\n",
    "    ALPHA = 0.7\n",
    "    N_LIMIT = 200000                \n",
    "\n",
    "    os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "    print(\"[sanity] VADER available:\", _VADER is not None)\n",
    "    print(\"[sanity] 'I love it' →\", _safe_vader_compound(\"I love it\"))\n",
    "    print(\"[sanity] 'I hate it' →\", _safe_vader_compound(\"I hate it\"))\n",
    "\n",
    "    print(\"[load] reading:\", INPUT)\n",
    "    df = build_reviews_df(INPUT, n_limit=N_LIMIT)\n",
    "    print(f\"[rows] {len(df):,} reviews after basic cleaning\")\n",
    "\n",
    "    df = compute_features(df, alpha=ALPHA)\n",
    "    print(\"[features] computed hybrid sentiment + style proxies + month bins\")\n",
    "\n",
    "    df_clean, monthly, traj = build_user_trajectories(df, min_reviews=MIN_REVIEWS)\n",
    "    print(f\"[users] {traj.shape[0]:,} users with ≥ {MIN_REVIEWS} reviews\")\n",
    "\n",
    "    df_clean.to_parquet(os.path.join(OUTDIR, \"reviews.parquet\"))\n",
    "    monthly.to_parquet(os.path.join(OUTDIR, \"reviews_monthly.parquet\"))\n",
    "    traj.to_parquet(os.path.join(OUTDIR, \"user_trajectories.parquet\"))\n",
    "    traj.head(1000).to_csv(os.path.join(OUTDIR, \"user_trajectories_preview.csv\"), index=False)\n",
    "\n",
    "    display_heads = (df_clean.head(), monthly.head(), traj.head())\n",
    "    print(display_heads)\n",
    "\n",
    "    n_samples = min(6, len(traj))\n",
    "    if n_samples > 0:\n",
    "        sample_users = traj.dropna(subset=[\"sent_hybrid_seq\"]).sample(n_samples, random_state=42)[\"user_id\"].tolist()\n",
    "        for uid in sample_users:\n",
    "            g = monthly[monthly[\"user_id\"] == uid].sort_values(\"month\")\n",
    "            plt.figure()\n",
    "            plt.plot(g[\"month\"].dt.to_pydatetime(), g[\"sent_hybrid_mean\"])\n",
    "            plt.title(f\"user {uid} — monthly hybrid sentiment\")\n",
    "            plt.xlabel(\"month\")\n",
    "            plt.ylabel(\"hybrid sentiment ([-1,1])\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = min(6, len(traj))\n",
    "rep_users = traj.dropna(subset=[\"sent_hybrid_seq\"]).sample(n_samples, random_state=42)[\"user_id\"].tolist()\n",
    "\n",
    "os.makedirs(\"figs\", exist_ok=True)\n",
    "\n",
    "for i, uid in enumerate(rep_users, start=1):\n",
    "    g = monthly[monthly[\"user_id\"] == uid].sort_values(\"month\")\n",
    "    plt.figure()\n",
    "    # FutureWarning-safe: use .values for x\n",
    "    plt.plot(g[\"month\"].values, g[\"sent_hybrid_mean\"])\n",
    "    plt.title(f\"user {uid} — monthly hybrid sentiment\")\n",
    "    plt.xlabel(\"month\")\n",
    "    plt.ylabel(\"hybrid sentiment ([-1,1])\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figs/user_traj_{i}.png\", dpi=220, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tables\", exist_ok=True)\n",
    "cols = [\"user_id\", \"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]\n",
    "head = pd.concat([\n",
    "    traj.sort_values(\"drift_slope\", ascending=False).head(5),\n",
    "    traj.sort_values(\"drift_slope\", ascending=True).head(5)\n",
    "])[cols].drop_duplicates().head(10)\n",
    "\n",
    "head = head.assign(\n",
    "    drift_slope=head[\"drift_slope\"].round(3),\n",
    "    drift_delta=head[\"drift_delta\"].round(3),\n",
    "    tv=head[\"tv\"].round(3),\n",
    "    flip_rate=head[\"flip_rate\"].round(3),\n",
    ")\n",
    "\n",
    "head.to_latex(\"tables/user_metrics.tex\", index=False, escape=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f67f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] created run directory: runs\\baseline_v1_20251110_183220\n",
      "[sanity] VADER available: True\n",
      "[sanity] 'I love it' → 0.6369\n",
      "[sanity] 'I hate it' → -0.5719\n",
      "[sanity] baseline config: {'dataset_name': 'Amazon Electronics 5-core', 'input_path': 'C:\\\\Users\\\\shubh\\\\OneDrive\\\\Desktop\\\\nlp\\\\Electronics_5.json\\\\Electronics_5.json', 'out_root': 'runs', 'run_name': 'baseline_v1', 'N_LIMIT': 200000, 'MIN_REVIEWS': 5, 'ALPHA': 0.7, 'seed': 42}\n",
      "[load] reading: C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\n",
      "[rows] 200,000 reviews after cleaning\n",
      "[features] computed hybrid sentiment + style proxies + month bins\n",
      "[users] 2,657 users with ≥ 5 reviews\n",
      "[save] wrote parquet + preview to runs\\baseline_v1_20251110_183220\\data\n",
      "[metrics] saved summary metrics to metrics_baseline.json\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_1.png\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_2.png\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_3.png\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_4.png\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_5.png\n",
      "[fig] saved runs\\baseline_v1_20251110_183220\\figs\\user_traj_6.png\n",
      "[done] baseline run complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Baseline pipeline for ReviewMirror\n",
    "\"\"\"\n",
    "\n",
    "# SECTION 0 — Imports & setup\n",
    "\n",
    "import os, gzip, json, re, datetime as dt\n",
    "from typing import Iterator, Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sentiment (VADER)\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    _VADER = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    _VADER = None\n",
    "\n",
    "_WORD = re.compile(r\"\\b\\w+\\b\")\n",
    "_FIRST_PERSON = re.compile(r\"\\b(i|i'm|i’ve|i've|my|me)\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# SECTION 1 — Reader for Amazon/Yelp formats\n",
    "\n",
    "def _open_text(path: str):\n",
    "    if path.endswith(\".gz\"):\n",
    "        return gzip.open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"rt\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def parse_amz_reviews(path: str, n_limit: Optional[int] = None) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Yields dict reviews from:\n",
    "      - JSON Lines (usual Amazon format): one JSON object per line\n",
    "      - OR a single large JSON array of objects\n",
    "    \"\"\"\n",
    "    with _open_text(path) as f:\n",
    "        head = f.read(2048)\n",
    "        if not head:\n",
    "            return\n",
    "        is_array = head.lstrip().startswith(\"[\")\n",
    "\n",
    "    if is_array:\n",
    "        with _open_text(path) as f:\n",
    "            data = json.load(f)\n",
    "            for i, obj in enumerate(data):\n",
    "                yield obj\n",
    "                if n_limit is not None and i + 1 >= n_limit:\n",
    "                    break\n",
    "    else:\n",
    "        with _open_text(path) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                yield obj\n",
    "                if n_limit is not None and i + 1 >= n_limit:\n",
    "                    break\n",
    "\n",
    "\n",
    "# SECTION 2 — Feature helpers (VADER, mapping, style, utils)\n",
    "\n",
    "def _safe_vader_compound(text: str) -> float:\n",
    "    \"\"\"\n",
    "    VADER compound in [-1,1]. Uses a global analyzer if available, else a simple fallback.\n",
    "    \"\"\"\n",
    "    t = text or \"\"\n",
    "    if _VADER is not None:\n",
    "        try:\n",
    "            return float(_VADER.polarity_scores(t)[\"compound\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    tl = t.lower()\n",
    "    pos = sum(tl.count(w) for w in [\" good\", \" great\", \" excellent\", \" amazing\", \" love\", \" perfect\", \" happy\"])\n",
    "    neg = sum(tl.count(w) for w in [\" bad\", \" terrible\", \" awful\", \" hate\", \" poor\", \" broken\", \" angry\"])\n",
    "    return 0.0 if (pos + neg) == 0 else (pos - neg) / (pos + neg)\n",
    "\n",
    "def _stars_to_unit(stars: float) -> float:\n",
    "    \"\"\"Map 1..5 -> [-1, 1]\"\"\"\n",
    "    try:\n",
    "        s = float(stars)\n",
    "        return (s - 3.0) / 2.0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _month_floor(ts: pd.Timestamp) -> pd.Timestamp:\n",
    "    return pd.Timestamp(year=ts.year, month=ts.month, day=1, tz=\"UTC\")\n",
    "\n",
    "def _is_ascii_major(text) -> bool:\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    if not text:\n",
    "        return True\n",
    "    ascii_chars = sum(1 for ch in text if ord(ch) < 128)\n",
    "    return ascii_chars >= 0.9 * len(text)\n",
    "\n",
    "def _to_int_or_nan(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace(\",\", \"\").strip()\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _style_feats(txt: str) -> Tuple[float, float, float]:\n",
    "    \"\"\"(exclam_per_char, first_person_per_token, caps_per_char)\"\"\"\n",
    "    if not isinstance(txt, str) or not txt:\n",
    "        return (0.0, 0.0, 0.0)\n",
    "    n_chars = max(1, len(txt))\n",
    "    tokens = _WORD.findall(txt)\n",
    "    n_tok = max(1, len(tokens))\n",
    "    exclam = txt.count(\"!\") / n_chars\n",
    "    first_person = len(_FIRST_PERSON.findall(txt)) / n_tok\n",
    "    caps = sum(1 for ch in txt if ch.isupper()) / n_chars\n",
    "    return exclam, first_person, caps\n",
    "\n",
    "\n",
    "# SECTION 3 — Build row-level DataFrame\n",
    "\n",
    "def build_reviews_df(path: str, n_limit: Optional[int] = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for rec in parse_amz_reviews(path, n_limit=n_limit):\n",
    "        rows.append({\n",
    "            \"user_id\": rec.get(\"reviewerID\") or rec.get(\"user_id\"),\n",
    "            \"item_id\": rec.get(\"asin\") or rec.get(\"business_id\"),\n",
    "            \"text\": rec.get(\"reviewText\") or rec.get(\"text\"),\n",
    "            \"stars\": rec.get(\"overall\") or rec.get(\"stars\"),\n",
    "            \"ts\": pd.to_datetime(\n",
    "                rec.get(\"unixReviewTime\"), unit=\"s\", utc=True\n",
    "            ) if rec.get(\"unixReviewTime\") is not None else (\n",
    "                pd.to_datetime(rec.get(\"reviewTime\"), utc=True, errors=\"coerce\")\n",
    "                if rec.get(\"reviewTime\") is not None else\n",
    "                pd.to_datetime(rec.get(\"date\"), utc=True, errors=\"coerce\")\n",
    "            ),\n",
    "            \"summary\": rec.get(\"summary\"),\n",
    "            \"helpful_votes\": _to_int_or_nan(rec.get(\"vote\") or rec.get(\"useful\")),\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Clean\n",
    "    df = df.dropna(subset=[\"user_id\", \"item_id\", \"ts\"]).reset_index(drop=True)\n",
    "    df = df[df[\"text\"].map(_is_ascii_major)]\n",
    "    return df\n",
    "\n",
    "\n",
    "# SECTION 4 — Compute features (sentiment, style, month)\n",
    "\n",
    "def compute_features(df: pd.DataFrame, alpha: float = 0.7) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"sent_text\"]  = df[\"text\"].map(_safe_vader_compound).astype(float)\n",
    "    df[\"sent_stars\"] = df[\"stars\"].map(_stars_to_unit).astype(float)\n",
    "    df[\"sent_hybrid\"] = alpha * df[\"sent_text\"] + (1.0 - alpha) * df[\"sent_stars\"]\n",
    "\n",
    "    feats = df[\"text\"].map(_style_feats).tolist()\n",
    "    df[[\"style_exclam_rate\",\"style_firstperson_rate\",\"style_caps_rate\"]] = pd.DataFrame(\n",
    "        feats, index=df.index\n",
    "    )\n",
    "    df[\"month\"] = df[\"ts\"].map(_month_floor)\n",
    "    return df\n",
    "\n",
    "\n",
    "# SECTION 5 — Aggregate to per-user trajectories + drift stats\n",
    "\n",
    "def _group_apply_include_groups(grouper, func):\n",
    "    try:\n",
    "        return grouper.apply(func, include_groups=False)\n",
    "    except TypeError:\n",
    "        return grouper.apply(func)\n",
    "\n",
    "def build_user_trajectories(df: pd.DataFrame, min_reviews: int = 5):\n",
    "    df = df.sort_values([\"user_id\",\"ts\"]).copy()\n",
    "\n",
    "    # keep users with >= min_reviews\n",
    "    vc = df[\"user_id\"].value_counts()\n",
    "    keep = set(vc[vc >= min_reviews].index)\n",
    "    df = df[df[\"user_id\"].isin(keep)].copy()\n",
    "\n",
    "    df[\"text_z_user\"] = df.groupby(\"user_id\")[\"sent_text\"].transform(\n",
    "        lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # monthly aggregates\n",
    "    monthly = (df.groupby([\"user_id\",\"month\"])\n",
    "                 .agg(sent_text_mean=(\"sent_text\",\"mean\"),\n",
    "                      sent_hybrid_mean=(\"sent_hybrid\",\"mean\"),\n",
    "                      stars_mean=(\"stars\",\"mean\"),\n",
    "                      n_reviews=(\"text\",\"count\"))\n",
    "                 .reset_index())\n",
    "\n",
    "    # TRUE time for slope: month ordinal (YYYY*12 + MM)\n",
    "    monthly[\"month_ord\"] = monthly[\"month\"].dt.year * 12 + monthly[\"month\"].dt.month\n",
    "\n",
    "    # slope + delta per user (on true time)\n",
    "    def _slope_delta(g: pd.DataFrame) -> Dict[str, float]:\n",
    "        g = g.sort_values(\"month_ord\")\n",
    "        x = g[\"month_ord\"].values.astype(float)\n",
    "        y = g[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(x) < 2 or np.isnan(y).all():\n",
    "            return {\"drift_slope\": np.nan, \"drift_delta\": np.nan}\n",
    "        xc = x - x.mean()\n",
    "        denom = float(np.dot(xc, xc) + 1e-8)\n",
    "        slope = float(np.dot(xc, (y - y.mean())) / denom)\n",
    "        delta = float(y[-1] - y[0])\n",
    "        return {\"drift_slope\": slope, \"drift_delta\": delta}\n",
    "\n",
    "    drift = (_group_apply_include_groups(monthly.groupby(\"user_id\"), _slope_delta)\n",
    "                .apply(pd.Series)\n",
    "                .reset_index())\n",
    "\n",
    "    def _extra_metrics(g: pd.DataFrame) -> Dict[str, float]:\n",
    "        y = g.sort_values(\"month_ord\")[\"sent_hybrid_mean\"].values.astype(float)\n",
    "        if len(y) < 2 or np.isnan(y).all():\n",
    "            return {\"tv\": np.nan, \"flip_rate\": np.nan}\n",
    "        tv = float(np.nansum(np.abs(np.diff(y))))\n",
    "        flips = float(np.nansum(np.sign(y[1:]) != np.sign(y[:-1])))\n",
    "        flip_rate = flips / (len(y) - 1)\n",
    "        return {\"tv\": tv, \"flip_rate\": flip_rate}\n",
    "\n",
    "    extra = (_group_apply_include_groups(monthly.groupby(\"user_id\"), _extra_metrics)\n",
    "                .apply(pd.Series)\n",
    "                .reset_index())\n",
    "\n",
    "    seqs = (monthly.groupby(\"user_id\")\n",
    "                  .agg(months=(\"month\", list),\n",
    "                       sent_hybrid_seq=(\"sent_hybrid_mean\", list),\n",
    "                       stars_seq=(\"stars_mean\", list),\n",
    "                       counts_seq=(\"n_reviews\", list))\n",
    "                  .reset_index())\n",
    "\n",
    "    traj = pd.merge(drift, seqs, on=\"user_id\", how=\"left\")\n",
    "    traj = pd.merge(traj, extra, on=\"user_id\", how=\"left\")\n",
    "    return df, monthly, traj\n",
    "\n",
    "\n",
    "# SECTION 6 — Main: Config, run, save, plots\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"dataset_name\": \"Amazon Electronics 5-core\",\n",
    "        \"input_path\": r\"C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\",\n",
    "        \"out_root\": \"runs\",\n",
    "        \"run_name\": \"baseline_v1\",\n",
    "        \"N_LIMIT\": 200000,     \n",
    "        \"MIN_REVIEWS\": 5,\n",
    "        \"ALPHA\": 0.7,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    np.random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "    run_id = f'{CONFIG[\"run_name\"]}_{dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    RUN_DIR = os.path.join(CONFIG[\"out_root\"], run_id)\n",
    "    FIG_DIR = os.path.join(RUN_DIR, \"figs\")\n",
    "    DATA_DIR = os.path.join(RUN_DIR, \"data\")\n",
    "    os.makedirs(FIG_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"[run] created run directory: {RUN_DIR}\")\n",
    "\n",
    "    with open(os.path.join(RUN_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "    # 3) Load + preprocess\n",
    "    INPUT = CONFIG[\"input_path\"]\n",
    "    N_LIMIT = CONFIG[\"N_LIMIT\"]\n",
    "    MIN_REVIEWS = CONFIG[\"MIN_REVIEWS\"]\n",
    "    ALPHA = CONFIG[\"ALPHA\"]\n",
    "\n",
    "    print(\"[sanity] VADER available:\", _VADER is not None)\n",
    "    print(\"[sanity] 'I love it' →\", _safe_vader_compound(\"I love it\"))\n",
    "    print(\"[sanity] 'I hate it' →\", _safe_vader_compound(\"I hate it\"))\n",
    "\n",
    "    print(\"[sanity] baseline config:\", CONFIG)\n",
    "    print(\"[load] reading:\", INPUT)\n",
    "    df = build_reviews_df(INPUT, n_limit=N_LIMIT)\n",
    "    print(f\"[rows] {len(df):,} reviews after cleaning\")\n",
    "\n",
    "    df = compute_features(df, alpha=ALPHA)\n",
    "    print(\"[features] computed hybrid sentiment + style proxies + month bins\")\n",
    "\n",
    "    df_clean, monthly, traj = build_user_trajectories(df, min_reviews=MIN_REVIEWS)\n",
    "    print(f\"[users] {traj.shape[0]:,} users with ≥ {MIN_REVIEWS} reviews\")\n",
    "\n",
    "    df_clean.to_parquet(os.path.join(DATA_DIR, \"reviews.parquet\"))\n",
    "    monthly.to_parquet(os.path.join(DATA_DIR, \"reviews_monthly.parquet\"))\n",
    "    traj.to_parquet(os.path.join(DATA_DIR, \"user_trajectories.parquet\"))\n",
    "    traj.head(1000).to_csv(os.path.join(DATA_DIR, \"user_trajectories_preview.csv\"), index=False)\n",
    "    print(\"[save] wrote parquet + preview to\", DATA_DIR)\n",
    "\n",
    "    metrics = {\n",
    "        \"n_rows\": int(len(df_clean)),\n",
    "        \"n_users\": int(traj[\"user_id\"].nunique()),\n",
    "        \"n_items\": int(df_clean[\"item_id\"].nunique()),\n",
    "        \"min_reviews_per_user\": int(MIN_REVIEWS),\n",
    "        \"alpha_hybrid\": float(ALPHA),\n",
    "        \"stars_mean\": float(df_clean[\"stars\"].mean()),\n",
    "        \"stars_std\": float(df_clean[\"stars\"].std()),\n",
    "        \"sent_text_mean\": float(df_clean[\"sent_text\"].mean()),\n",
    "        \"sent_text_std\": float(df_clean[\"sent_text\"].std()),\n",
    "        \"sent_hybrid_mean\": float(df_clean[\"sent_hybrid\"].mean()),\n",
    "        \"sent_hybrid_std\": float(df_clean[\"sent_hybrid\"].std()),\n",
    "    }\n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]:\n",
    "        if col in traj.columns:\n",
    "            metrics[f\"{col}_mean\"] = float(traj[col].mean())\n",
    "            metrics[f\"{col}_std\"] = float(traj[col].std())\n",
    "            metrics[f\"{col}_min\"] = float(traj[col].min())\n",
    "            metrics[f\"{col}_max\"] = float(traj[col].max())\n",
    "\n",
    "    with open(os.path.join(RUN_DIR, \"metrics_baseline.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"[metrics] saved summary metrics to metrics_baseline.json\")\n",
    "\n",
    "    n_samples = min(6, len(traj))\n",
    "    if n_samples > 0:\n",
    "        sample_users = traj.dropna(subset=[\"sent_hybrid_seq\"]).sample(\n",
    "            n_samples, random_state=CONFIG[\"seed\"]\n",
    "        )[\"user_id\"].tolist()\n",
    "\n",
    "        for i, uid in enumerate(sample_users, start=1):\n",
    "            g = monthly[monthly[\"user_id\"] == uid].sort_values(\"month\")\n",
    "            plt.figure()\n",
    "            # FutureWarning-safe: np.array(...)\n",
    "            plt.plot(np.array(g[\"month\"].values), g[\"sent_hybrid_mean\"])\n",
    "            plt.title(f\"user {uid} — monthly hybrid sentiment\")\n",
    "            plt.xlabel(\"month\")\n",
    "            plt.ylabel(\"hybrid sentiment ([-1,1])\")\n",
    "            plt.tight_layout()\n",
    "            fig_path = os.path.join(FIG_DIR, f\"user_traj_{i}.png\")\n",
    "            plt.savefig(fig_path, dpi=220, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"[fig] saved {fig_path}\")\n",
    "\n",
    "    print(\"[done] baseline run complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d07133",
   "metadata": {},
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c070d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 7 — Data Splits for Evaluation\n",
    "\n",
    "def create_data_splits(traj: pd.DataFrame, monthly: pd.DataFrame, \n",
    "                       split_ratio=(0.7, 0.15, 0.15), strategy=\"temporal\"):\n",
    "    \"\"\"\n",
    "    Split users into train/val/test sets for reproducible evaluation.\n",
    "    \n",
    "    Args:\n",
    "        traj: User trajectories dataframe\n",
    "        monthly: Monthly aggregated reviews\n",
    "        split_ratio: (train, val, test) proportions\n",
    "        strategy: 'temporal' or 'random'\n",
    "    \n",
    "    Returns:\n",
    "        Dict with 'train', 'val', 'test' user lists\n",
    "    \"\"\"\n",
    "    if strategy == \"temporal\":\n",
    "        user_first_review = monthly.groupby(\"user_id\")[\"month\"].min()\n",
    "        sorted_users = user_first_review.sort_values().index.tolist()\n",
    "    else:\n",
    "        sorted_users = traj[\"user_id\"].sample(frac=1, random_state=42).tolist()\n",
    "    \n",
    "    n = len(sorted_users)\n",
    "    train_end = int(n * split_ratio[0])\n",
    "    val_end = train_end + int(n * split_ratio[1])\n",
    "    \n",
    "    return {\n",
    "        \"train\": sorted_users[:train_end],\n",
    "        \"val\": sorted_users[train_end:val_end],\n",
    "        \"test\": sorted_users[val_end:]\n",
    "    }\n",
    "\n",
    "\n",
    "# SECTION 8 — Baseline Metrics & Evaluation\n",
    "\n",
    "def compute_baseline_statistics(traj: pd.DataFrame, splits: Dict):\n",
    "    \"\"\"\n",
    "    Compute baseline statistics from training set.\n",
    "    These serve as naive predictors (e.g., \"predict mean drift for all users\").\n",
    "    \"\"\"\n",
    "    train_traj = traj[traj[\"user_id\"].isin(splits[\"train\"])]\n",
    "    \n",
    "    baselines = {\n",
    "        \"mean_drift_slope\": float(train_traj[\"drift_slope\"].mean()),\n",
    "        \"mean_drift_delta\": float(train_traj[\"drift_delta\"].mean()),\n",
    "        \"median_drift_slope\": float(train_traj[\"drift_slope\"].median()),\n",
    "        \"median_drift_delta\": float(train_traj[\"drift_delta\"].median()),\n",
    "        \"std_drift_slope\": float(train_traj[\"drift_slope\"].std()),\n",
    "        \"std_drift_delta\": float(train_traj[\"drift_delta\"].std()),\n",
    "    }\n",
    "    \n",
    "    return baselines\n",
    "\n",
    "\n",
    "def evaluate_drift_metrics(traj: pd.DataFrame, splits: Dict, baselines: Dict):\n",
    "    \"\"\"\n",
    "    Evaluate drift detection on test set.\n",
    "    Reports distribution stats + how far users deviate from baseline assumptions.\n",
    "    \"\"\"\n",
    "    test_traj = traj[traj[\"user_id\"].isin(splits[\"test\"])]\n",
    "    \n",
    "    metrics = {\n",
    "        \"test_size\": int(len(test_traj)),\n",
    "        \n",
    "        \"slope_mae\": float(np.abs(test_traj[\"drift_slope\"] - baselines[\"mean_drift_slope\"]).mean()),\n",
    "        \"delta_mae\": float(np.abs(test_traj[\"drift_delta\"] - baselines[\"mean_drift_delta\"]).mean()),\n",
    "        \n",
    "        \"high_drift_users\": int((np.abs(test_traj[\"drift_slope\"]) > 0.01).sum()),\n",
    "        \"volatile_users\": int((test_traj[\"tv\"] > test_traj[\"tv\"].quantile(0.75)).sum()),\n",
    "        \"flat_users\": int((np.abs(test_traj[\"drift_slope\"]) < 0.001).sum()),\n",
    "    }\n",
    "    \n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]:\n",
    "        if col in test_traj.columns:\n",
    "            metrics[f\"test_{col}_mean\"] = float(test_traj[col].mean())\n",
    "            metrics[f\"test_{col}_median\"] = float(test_traj[col].median())\n",
    "            metrics[f\"test_{col}_std\"] = float(test_traj[col].std())\n",
    "            metrics[f\"test_{col}_q25\"] = float(test_traj[col].quantile(0.25))\n",
    "            metrics[f\"test_{col}_q75\"] = float(test_traj[col].quantile(0.75))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# SECTION 9 — Enhanced Visualizations\n",
    "\n",
    "def create_diagnostic_plots(traj: pd.DataFrame, monthly: pd.DataFrame, fig_dir: str):\n",
    "    \n",
    "    # 1. Distribution of drift metrics (4-panel)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    drift_cols = [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]\n",
    "    \n",
    "    for ax, col in zip(axes.flat, drift_cols):\n",
    "        data = traj[col].dropna()\n",
    "        ax.hist(data, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "        ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {data.mean():.3f}')\n",
    "        ax.axvline(data.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: {data.median():.3f}')\n",
    "        ax.set_title(f\"Distribution of {col}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"drift_distributions.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    print(f\"[fig] saved {os.path.join(fig_dir, 'drift_distributions.png')}\")\n",
    "    \n",
    "    monthly_agg = monthly.groupby(\"month\").agg({\n",
    "        \"sent_hybrid_mean\": \"mean\",\n",
    "        \"stars_mean\": \"mean\",\n",
    "        \"n_reviews\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    ax1.plot(monthly_agg[\"month\"], monthly_agg[\"sent_hybrid_mean\"], \n",
    "             marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "    ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax1.set_title(\"Average Hybrid Sentiment Over Time (All Users)\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel(\"Mean Sentiment\", fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.bar(monthly_agg[\"month\"], monthly_agg[\"n_reviews\"], alpha=0.7, color='coral')\n",
    "    ax2.set_title(\"Review Volume Over Time\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Month\", fontsize=11)\n",
    "    ax2.set_ylabel(\"Number of Reviews\", fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"temporal_trends.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    print(f\"[fig] saved {os.path.join(fig_dir, 'temporal_trends.png')}\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(traj[\"drift_slope\"], traj[\"tv\"], alpha=0.4, s=20)\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(traj[\"tv\"].median(), color='orange', linestyle='--', alpha=0.5, \n",
    "                label=f'Median TV: {traj[\"tv\"].median():.2f}')\n",
    "    plt.xlabel(\"Drift Slope (sentiment change rate)\", fontsize=11)\n",
    "    plt.ylabel(\"Total Variation (volatility)\", fontsize=11)\n",
    "    plt.title(\"Relationship: Drift Slope vs. Volatility\", fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"slope_vs_volatility.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    print(f\"[fig] saved {os.path.join(fig_dir, 'slope_vs_volatility.png')}\")\n",
    "\n",
    "\n",
    "# SECTION 10 — Reproducibility Utilities\n",
    "\n",
    "def create_reproducibility_manifest(run_dir: str, config: dict, df: pd.DataFrame):\n",
    "    \"\"\"Document all information needed to reproduce this run.\"\"\"\n",
    "    import sys\n",
    "    \n",
    "    manifest = {\n",
    "        \"run_info\": {\n",
    "            \"run_id\": os.path.basename(run_dir),\n",
    "            \"timestamp\": dt.datetime.now().isoformat(),\n",
    "            \"python_version\": sys.version,\n",
    "            \"package_versions\": {\n",
    "                \"pandas\": pd.__version__,\n",
    "                \"numpy\": np.__version__,\n",
    "                \"matplotlib\": plt.matplotlib.__version__,\n",
    "            }\n",
    "        },\n",
    "        \"config\": config,\n",
    "        \"data_summary\": {\n",
    "            \"total_reviews\": int(len(df)),\n",
    "            \"date_range\": {\n",
    "                \"min\": str(df[\"ts\"].min()),\n",
    "                \"max\": str(df[\"ts\"].max()),\n",
    "                \"span_days\": int((df[\"ts\"].max() - df[\"ts\"].min()).days)\n",
    "            },\n",
    "            \"unique_users\": int(df[\"user_id\"].nunique()),\n",
    "            \"unique_items\": int(df[\"item_id\"].nunique()),\n",
    "        },\n",
    "        \"preprocessing_pipeline\": [\n",
    "            \"1. Load JSON/JSONL reviews (gzip-compatible)\",\n",
    "            \"2. Parse reviewerID/asin, reviewText, overall, unixReviewTime\",\n",
    "            \"3. Filter: drop rows with missing user_id, item_id, or timestamp\",\n",
    "            \"4. Filter: keep only reviews with >=90% ASCII characters\",\n",
    "            \"5. Compute VADER sentiment on review text → sent_text\",\n",
    "            \"6. Normalize star ratings (1-5) to [-1,+1] → sent_stars\",\n",
    "            f\"7. Create hybrid sentiment: {config['ALPHA']}*sent_text + {1-config['ALPHA']}*sent_stars\",\n",
    "            \"8. Extract style features: exclamation rate, first-person rate, caps rate\",\n",
    "            \"9. Aggregate to monthly bins per user\",\n",
    "            f\"10. Keep only users with >={config['MIN_REVIEWS']} reviews\",\n",
    "            \"11. Compute drift metrics: slope (linear), delta (first-to-last), TV, flip_rate\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(run_dir, \"manifest.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    print(f\"[manifest] saved to {os.path.join(run_dir, 'manifest.json')}\")\n",
    "\n",
    "\n",
    "def create_run_readme(run_dir: str, config: dict, metrics: dict):\n",
    "    readme = f\"\"\"# ReviewMirror Baseline Run\n",
    "\n",
    "**Run ID**: `{os.path.basename(run_dir)}`  \n",
    "**Date**: {dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Dataset | {config['dataset_name']} |\n",
    "| Reviews Analyzed | {config['N_LIMIT'] if config['N_LIMIT'] else 'ALL'} |\n",
    "| Min Reviews/User | {config['MIN_REVIEWS']} |\n",
    "| Hybrid Alpha (text weight) | {config['ALPHA']} |\n",
    "| Random Seed | {config['seed']} |\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Data Summary\n",
    "\n",
    "- **Total reviews** (after cleaning): {metrics.get('n_rows', 'N/A'):,}\n",
    "- **Unique users**: {metrics.get('n_users', 'N/A'):,}\n",
    "- **Unique items**: {metrics.get('n_items', 'N/A'):,}\n",
    "- **Average stars**: {metrics.get('stars_mean', 0):.2f} ± {metrics.get('stars_std', 0):.2f}\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Key Findings\n",
    "\n",
    "### Drift Metrics (All Users)\n",
    "| Metric | Mean | Std | Min | Max |\n",
    "|--------|------|-----|-----|-----|\n",
    "| Drift Slope | {metrics.get('drift_slope_mean', 0):.4f} | {metrics.get('drift_slope_std', 0):.4f} | {metrics.get('drift_slope_min', 0):.4f} | {metrics.get('drift_slope_max', 0):.4f} |\n",
    "| Drift Delta | {metrics.get('drift_delta_mean', 0):.4f} | {metrics.get('drift_delta_std', 0):.4f} | {metrics.get('drift_delta_min', 0):.4f} | {metrics.get('drift_delta_max', 0):.4f} |\n",
    "| Total Variation | {metrics.get('tv_mean', 0):.4f} | {metrics.get('tv_std', 0):.4f} | {metrics.get('tv_min', 0):.4f} | {metrics.get('tv_max', 0):.4f} |\n",
    "\n",
    "### Interesting User Counts\n",
    "- **High-drift users** (|slope| > 0.01): {metrics.get('high_drift_users', 'N/A')}\n",
    "- **Volatile users** (TV > 75th percentile): {metrics.get('volatile_users', 'N/A')}\n",
    "- **Flat users** (|slope| < 0.001): {metrics.get('flat_users', 'N/A')}\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Output Files\n",
    "```\n",
    "{os.path.basename(run_dir)}/\n",
    "├── data/\n",
    "│   ├── reviews.parquet              # Review-level data with features\n",
    "│   ├── reviews_monthly.parquet      # Monthly aggregates per user\n",
    "│   ├── user_trajectories.parquet    # Per-user drift metrics\n",
    "│   ├── splits.json                  # Train/val/test split\n",
    "│   └── user_trajectories_preview.csv\n",
    "├── figs/\n",
    "│   ├── drift_distributions.png      # Histograms of drift metrics\n",
    "│   ├── temporal_trends.png          # Sentiment over time\n",
    "│   ├── slope_vs_volatility.png      # Correlation plot\n",
    "│   └── user_traj_*.png              # Sample user trajectories\n",
    "├── config.json                       # Full configuration\n",
    "├── metrics_baseline.json             # Quantitative metrics\n",
    "├── manifest.json                     # Reproducibility info\n",
    "└── README.md                         # This file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Modeling Rationale\n",
    "\n",
    "### Why Hybrid Sentiment?\n",
    "Combines two complementary signals:\n",
    "- **Text sentiment** (VADER): Captures nuanced language (e.g., \"good but...\")\n",
    "- **Star ratings**: Captures user's explicit judgment\n",
    "\n",
    "Hybrid formula: `{config['ALPHA']} × text + {1-config['ALPHA']} × stars`\n",
    "\n",
    "### Why These Drift Metrics?\n",
    "\n",
    "1. **Drift Slope**: Linear trend coefficient\n",
    "   - Robust to noise via least-squares fitting\n",
    "   - Interpretable: \"sentiment changes by X per month\"\n",
    "\n",
    "2. **Drift Delta**: First − Last sentiment\n",
    "   - Captures endpoint difference\n",
    "   - Complementary to slope (non-linear changes)\n",
    "\n",
    "3. **Total Variation (TV)**: Sum of absolute month-to-month changes\n",
    "   - Measures volatility/instability\n",
    "   - Independent of overall direction\n",
    "\n",
    "4. **Flip Rate**: Proportion of sign changes\n",
    "   - Detects erratic behavior\n",
    "   - Normalized for trajectory length\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "- [ ] Experiment with different `ALPHA` values (text vs. stars weight)\n",
    "- [ ] Try transformer-based sentiment (e.g., fine-tuned BERT)\n",
    "- [ ] Cluster users by drift pattern (k-means on slope/TV)\n",
    "- [ ] Build predictive model: forecast future drift from early reviews\n",
    "- [ ] Analyze item-level effects (do certain products induce drift?)\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Notes\n",
    "\n",
    "This is a **statistical baseline** for ReviewMirror. No machine learning models trained yet.\n",
    "Goal: Establish ground truth about drift patterns in the data before building predictive systems.\n",
    "\"\"\"\n",
    "    \n",
    "    with open(os.path.join(run_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "    print(f\"[readme] saved to {os.path.join(run_dir, 'README.md')}\")\n",
    "\n",
    "\n",
    "# SECTION 11 — Sanity Checks\n",
    "\n",
    "def run_sanity_checks(df: pd.DataFrame, traj: pd.DataFrame):\n",
    "    issues = []\n",
    "    \n",
    "    nan_frac = df[\"sent_hybrid\"].isna().sum() / len(df)\n",
    "    if nan_frac > 0.1:\n",
    "        issues.append(f\"WARNING: {nan_frac:.1%} NaN in hybrid sentiment\")\n",
    "    \n",
    "    date_span = (df[\"ts\"].max() - df[\"ts\"].min()).days\n",
    "    if date_span < 30:\n",
    "        issues.append(f\"WARNING: Data spans only {date_span} days\")\n",
    "    \n",
    "    max_slope = traj[\"drift_slope\"].abs().max()\n",
    "    if max_slope > 1.0:\n",
    "        issues.append(f\"WARNING: Extreme drift_slope detected: {max_slope:.3f}\")\n",
    "    \n",
    "    max_reviews = df.groupby(\"user_id\").size().max()\n",
    "    median_reviews = df.groupby(\"user_id\").size().median()\n",
    "    if max_reviews > median_reviews * 20:\n",
    "        issues.append(f\"WARNING: Max reviews per user ({max_reviews}) >> median ({median_reviews})\")\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf85d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] created run directory: runs\\baseline_v1_20251110_195108\n",
      "[sanity] VADER available: True\n",
      "[sanity] 'I love it' → 0.6369\n",
      "[sanity] 'I hate it' → -0.5719\n",
      "[config] {'dataset_name': 'Amazon Electronics 5-core', 'input_path': 'C:\\\\Users\\\\shubh\\\\OneDrive\\\\Desktop\\\\nlp\\\\Electronics_5.json\\\\Electronics_5.json', 'out_root': 'runs', 'run_name': 'baseline_v1', 'N_LIMIT': 200000, 'MIN_REVIEWS': 5, 'ALPHA': 0.7, 'seed': 42, 'split_strategy': 'temporal', 'split_ratio': [0.7, 0.15, 0.15]}\n",
      "[load] reading: C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\n",
      "[rows] 200,000 reviews after cleaning\n",
      "[features] computed hybrid sentiment + style proxies + month bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_19956\\506542110.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_slope_delta)\n",
      "C:\\Users\\shubh\\AppData\\Local\\Temp\\ipykernel_19956\\506542110.py:58: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(_extra_metrics)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[users] 2,657 users with ≥ 5 reviews\n",
      "[check] WARNING: Extreme drift_slope detected: 1.275\n",
      "[check] WARNING: Max reviews per user (149) >> median (6.0)\n",
      "[splits] Train: 1859, Val: 398, Test: 400\n",
      "[baseline] Mean drift slope: 0.0005\n",
      "[baseline] Median drift slope: -0.0002\n",
      "[save] wrote parquet + preview to runs\\baseline_v1_20251110_195108\\data\n",
      "[metrics] saved summary + evaluation metrics\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\drift_distributions.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\temporal_trends.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\slope_vs_volatility.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_1.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_2.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_3.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_4.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_5.png\n",
      "[fig] saved runs\\baseline_v1_20251110_195108\\figs\\user_traj_6.png\n",
      "[manifest] saved to runs\\baseline_v1_20251110_195108\\manifest.json\n",
      "[readme] saved to runs\\baseline_v1_20251110_195108\\README.md\n",
      "\n",
      "============================================================\n",
      "[DONE] Baseline run complete!\n",
      "[DONE] All outputs saved to: runs\\baseline_v1_20251110_195108\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SECTION 12 — Main Pipeline \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG = {\n",
    "        \"dataset_name\": \"Amazon Electronics 5-core\",\n",
    "        \"input_path\": r\"C:\\Users\\shubh\\OneDrive\\Desktop\\nlp\\Electronics_5.json\\Electronics_5.json\",\n",
    "        \"out_root\": \"runs\",\n",
    "        \"run_name\": \"baseline_v1\",\n",
    "        \"N_LIMIT\": 200000,\n",
    "        \"MIN_REVIEWS\": 5,\n",
    "        \"ALPHA\": 0.7,\n",
    "        \"seed\": 42,\n",
    "        \"split_strategy\": \"temporal\",  # NEW\n",
    "        \"split_ratio\": [0.7, 0.15, 0.15]  # NEW: train/val/test\n",
    "    }\n",
    "    np.random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "    run_id = f'{CONFIG[\"run_name\"]}_{dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    RUN_DIR = os.path.join(CONFIG[\"out_root\"], run_id)\n",
    "    FIG_DIR = os.path.join(RUN_DIR, \"figs\")\n",
    "    DATA_DIR = os.path.join(RUN_DIR, \"data\")\n",
    "    os.makedirs(FIG_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"[run] created run directory: {RUN_DIR}\")\n",
    "\n",
    "    with open(os.path.join(RUN_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "    INPUT = CONFIG[\"input_path\"]\n",
    "    N_LIMIT = CONFIG[\"N_LIMIT\"]\n",
    "    MIN_REVIEWS = CONFIG[\"MIN_REVIEWS\"]\n",
    "    ALPHA = CONFIG[\"ALPHA\"]\n",
    "\n",
    "    print(\"[sanity] VADER available:\", _VADER is not None)\n",
    "    print(\"[sanity] 'I love it' →\", _safe_vader_compound(\"I love it\"))\n",
    "    print(\"[sanity] 'I hate it' →\", _safe_vader_compound(\"I hate it\"))\n",
    "    print(\"[config]\", CONFIG)\n",
    "\n",
    "    print(f\"[load] reading: {INPUT}\")\n",
    "    df = build_reviews_df(INPUT, n_limit=N_LIMIT)\n",
    "    print(f\"[rows] {len(df):,} reviews after cleaning\")\n",
    "\n",
    "    df = compute_features(df, alpha=ALPHA)\n",
    "    print(\"[features] computed hybrid sentiment + style proxies + month bins\")\n",
    "\n",
    "    df_clean, monthly, traj = build_user_trajectories(df, min_reviews=MIN_REVIEWS)\n",
    "    print(f\"[users] {traj.shape[0]:,} users with ≥ {MIN_REVIEWS} reviews\")\n",
    "\n",
    "    issues = run_sanity_checks(df_clean, traj)\n",
    "    for issue in issues:\n",
    "        print(f\"[check] {issue}\")\n",
    "\n",
    "    splits = create_data_splits(traj, monthly, \n",
    "                                split_ratio=CONFIG[\"split_ratio\"], \n",
    "                                strategy=CONFIG[\"split_strategy\"])\n",
    "    print(f\"[splits] Train: {len(splits['train'])}, Val: {len(splits['val'])}, Test: {len(splits['test'])}\")\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, \"splits.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({k: list(v) for k, v in splits.items()}, f, indent=2)\n",
    "\n",
    "    baselines = compute_baseline_statistics(traj, splits)\n",
    "    print(f\"[baseline] Mean drift slope: {baselines['mean_drift_slope']:.4f}\")\n",
    "    print(f\"[baseline] Median drift slope: {baselines['median_drift_slope']:.4f}\")\n",
    "    \n",
    "    with open(os.path.join(RUN_DIR, \"baselines.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(baselines, f, indent=2)\n",
    "\n",
    "    df_clean.to_parquet(os.path.join(DATA_DIR, \"reviews.parquet\"))\n",
    "    monthly.to_parquet(os.path.join(DATA_DIR, \"reviews_monthly.parquet\"))\n",
    "    traj.to_parquet(os.path.join(DATA_DIR, \"user_trajectories.parquet\"))\n",
    "    traj.head(1000).to_csv(os.path.join(DATA_DIR, \"user_trajectories_preview.csv\"), index=False)\n",
    "    print(\"[save] wrote parquet + preview to\", DATA_DIR)\n",
    "\n",
    "    metrics = {\n",
    "        \"n_rows\": int(len(df_clean)),\n",
    "        \"n_users\": int(traj[\"user_id\"].nunique()),\n",
    "        \"n_items\": int(df_clean[\"item_id\"].nunique()),\n",
    "        \"min_reviews_per_user\": int(MIN_REVIEWS),\n",
    "        \"alpha_hybrid\": float(ALPHA),\n",
    "        \"stars_mean\": float(df_clean[\"stars\"].mean()),\n",
    "        \"stars_std\": float(df_clean[\"stars\"].std()),\n",
    "        \"sent_text_mean\": float(df_clean[\"sent_text\"].mean()),\n",
    "        \"sent_text_std\": float(df_clean[\"sent_text\"].std()),\n",
    "        \"sent_hybrid_mean\": float(df_clean[\"sent_hybrid\"].mean()),\n",
    "        \"sent_hybrid_std\": float(df_clean[\"sent_hybrid\"].std()),\n",
    "    }\n",
    "    \n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]:\n",
    "        if col in traj.columns:\n",
    "            metrics[f\"{col}_mean\"] = float(traj[col].mean())\n",
    "            metrics[f\"{col}_std\"] = float(traj[col].std())\n",
    "            metrics[f\"{col}_min\"] = float(traj[col].min())\n",
    "            metrics[f\"{col}_max\"] = float(traj[col].max())\n",
    "\n",
    "    eval_metrics = evaluate_drift_metrics(traj, splits, baselines)\n",
    "    metrics.update(eval_metrics)\n",
    "    \n",
    "    with open(os.path.join(RUN_DIR, \"metrics_baseline.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"[metrics] saved summary + evaluation metrics\")\n",
    "\n",
    "    create_diagnostic_plots(traj, monthly, FIG_DIR)\n",
    "\n",
    "    n_samples = min(6, len(traj))\n",
    "    if n_samples > 0:\n",
    "        sample_users = traj.dropna(subset=[\"sent_hybrid_seq\"]).sample(\n",
    "            n_samples, random_state=CONFIG[\"seed\"]\n",
    "        )[\"user_id\"].tolist()\n",
    "\n",
    "        for i, uid in enumerate(sample_users, start=1):\n",
    "            g = monthly[monthly[\"user_id\"] == uid].sort_values(\"month\")\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(np.array(g[\"month\"].values), g[\"sent_hybrid_mean\"], \n",
    "                     marker='o', linewidth=2, markersize=6)\n",
    "            plt.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "            plt.title(f\"User {uid} — Monthly Hybrid Sentiment Trajectory\", \n",
    "                     fontsize=13, fontweight='bold')\n",
    "            plt.xlabel(\"Month\", fontsize=11)\n",
    "            plt.ylabel(\"Hybrid Sentiment ([-1, 1])\", fontsize=11)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            fig_path = os.path.join(FIG_DIR, f\"user_traj_{i}.png\")\n",
    "            plt.savefig(fig_path, dpi=220, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "            print(f\"[fig] saved {fig_path}\")\n",
    "\n",
    "    create_reproducibility_manifest(RUN_DIR, CONFIG, df_clean)\n",
    "    create_run_readme(RUN_DIR, CONFIG, metrics)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[DONE] Baseline run complete!\")\n",
    "    print(f\"[DONE] All outputs saved to: {RUN_DIR}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef4bd9",
   "metadata": {},
   "source": [
    "neww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33971aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_splits(traj: pd.DataFrame, monthly: pd.DataFrame, \n",
    "                       split_ratio=(0.7, 0.15, 0.15), strategy=\"temporal\"):\n",
    "    if strategy == \"temporal\":\n",
    "        # Sort by first review date per user\n",
    "        user_first_review = monthly.groupby(\"user_id\")[\"month\"].min()\n",
    "        sorted_users = user_first_review.sort_values().index.tolist()\n",
    "    else:\n",
    "        sorted_users = traj[\"user_id\"].sample(frac=1, random_state=42).tolist()\n",
    "    \n",
    "    n = len(sorted_users)\n",
    "    train_end = int(n * split_ratio[0])\n",
    "    val_end = train_end + int(n * split_ratio[1])\n",
    "    \n",
    "    return {\n",
    "        \"train\": sorted_users[:train_end],\n",
    "        \"val\": sorted_users[train_end:val_end],\n",
    "        \"test\": sorted_users[val_end:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b23280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baselines(traj: pd.DataFrame, splits: Dict):\n",
    "    \"\"\"\n",
    "    Compute simple baseline predictions:\n",
    "    - Random: random drift direction\n",
    "    - Mean: dataset mean drift\n",
    "    - Last-value: assume no change from last observation\n",
    "    \"\"\"\n",
    "    train_traj = traj[traj[\"user_id\"].isin(splits[\"train\"])]\n",
    "    \n",
    "    baselines = {\n",
    "        \"mean_drift_slope\": train_traj[\"drift_slope\"].mean(),\n",
    "        \"mean_drift_delta\": train_traj[\"drift_delta\"].mean(),\n",
    "        \"std_drift_slope\": train_traj[\"drift_slope\"].std(),\n",
    "        \"std_drift_delta\": train_traj[\"drift_delta\"].std(),\n",
    "    }\n",
    "    \n",
    "    # Save baselines\n",
    "    return baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5783ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_drift_metrics(traj: pd.DataFrame, splits: Dict, baselines: Dict):\n",
    "    \"\"\"\n",
    "    Evaluate drift detection performance on test set\n",
    "    \"\"\"\n",
    "    test_traj = traj[traj[\"user_id\"].isin(splits[\"test\"])]\n",
    "    \n",
    "    metrics = {\n",
    "        \"test_size\": len(test_traj),\n",
    "        \"slope_mae\": np.abs(test_traj[\"drift_slope\"] - baselines[\"mean_drift_slope\"]).mean(),\n",
    "        \"delta_mae\": np.abs(test_traj[\"drift_delta\"] - baselines[\"mean_drift_delta\"]).mean(),\n",
    "        \"slope_rmse\": np.sqrt(((test_traj[\"drift_slope\"] - baselines[\"mean_drift_slope\"])**2).mean()),\n",
    "        \"high_drift_users\": (np.abs(test_traj[\"drift_slope\"]) > 0.01).sum(),\n",
    "        \"volatile_users\": (test_traj[\"tv\"] > test_traj[\"tv\"].quantile(0.75)).sum(),\n",
    "    }\n",
    "    \n",
    "    # Distribution statistics\n",
    "    for col in [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]:\n",
    "        metrics[f\"test_{col}_mean\"] = test_traj[col].mean()\n",
    "        metrics[f\"test_{col}_median\"] = test_traj[col].median()\n",
    "        metrics[f\"test_{col}_q25\"] = test_traj[col].quantile(0.25)\n",
    "        metrics[f\"test_{col}_q75\"] = test_traj[col].quantile(0.75)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e054cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def setup_logging(run_dir: str):\n",
    "    \"\"\"Setup logging to both file and console\"\"\"\n",
    "    log_path = os.path.join(run_dir, \"pipeline.log\")\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_path),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# Then use: logger.info(f\"[load] reading: {INPUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9821e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diagnostic_plots(traj: pd.DataFrame, monthly: pd.DataFrame, fig_dir: str):\n",
    "    \n",
    "    # 1. Distribution of drift metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    for ax, col in zip(axes.flat, [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]):\n",
    "        ax.hist(traj[col].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax.set_title(f\"Distribution of {col}\")\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"drift_distributions.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Correlation matrix\n",
    "    corr_cols = [\"drift_slope\", \"drift_delta\", \"tv\", \"flip_rate\"]\n",
    "    corr_matrix = traj[corr_cols].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.xticks(range(len(corr_cols)), corr_cols, rotation=45)\n",
    "    plt.yticks(range(len(corr_cols)), corr_cols)\n",
    "    for i in range(len(corr_cols)):\n",
    "        for j in range(len(corr_cols)):\n",
    "            plt.text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                    ha='center', va='center')\n",
    "    plt.title(\"Correlation Matrix of Drift Metrics\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"correlation_matrix.png\"), dpi=220)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Time series of monthly sentiment (aggregated)\n",
    "    monthly_agg = monthly.groupby(\"month\").agg({\n",
    "        \"sent_hybrid_mean\": \"mean\",\n",
    "        \"stars_mean\": \"mean\",\n",
    "        \"n_reviews\": \"sum\"\n",
    "    }).reset_index()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    ax1.plot(monthly_agg[\"month\"], monthly_agg[\"sent_hybrid_mean\"], marker='o')\n",
    "    ax1.set_title(\"Average Hybrid Sentiment Over Time\")\n",
    "    ax1.set_ylabel(\"Mean Sentiment\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.bar(monthly_agg[\"month\"], monthly_agg[\"n_reviews\"], alpha=0.7)\n",
    "    ax2.set_title(\"Review Volume Over Time\")\n",
    "    ax2.set_xlabel(\"Month\")\n",
    "    ax2.set_ylabel(\"Number of Reviews\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, \"temporal_trends.png\"), dpi=220)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de049030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def statistical_tests(traj: pd.DataFrame):\n",
    "    results = {}\n",
    "    \n",
    "    # Test if drift_slope significantly different from 0\n",
    "    t_stat, p_val = stats.ttest_1samp(traj[\"drift_slope\"].dropna(), 0)\n",
    "    results[\"slope_vs_zero\"] = {\"t_statistic\": float(t_stat), \"p_value\": float(p_val)}\n",
    "    \n",
    "    # Correlation between slope and volatility\n",
    "    corr, p_val = stats.pearsonr(\n",
    "        traj[\"drift_slope\"].dropna(), \n",
    "        traj[\"tv\"].dropna()\n",
    "    )\n",
    "    results[\"slope_tv_correlation\"] = {\"correlation\": float(corr), \"p_value\": float(p_val)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ef9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reproducibility_manifest(run_dir: str, config: dict, df: pd.DataFrame):\n",
    "    \"\"\"Document everything needed to reproduce the run\"\"\"\n",
    "    manifest = {\n",
    "        \"run_info\": {\n",
    "            \"run_id\": os.path.basename(run_dir),\n",
    "            \"timestamp\": dt.datetime.now().isoformat(),\n",
    "            \"python_version\": sys.version,\n",
    "            \"package_versions\": {\n",
    "                \"pandas\": pd.__version__,\n",
    "                \"numpy\": np.__version__,\n",
    "                # Add other packages\n",
    "            }\n",
    "        },\n",
    "        \"config\": config,\n",
    "        \"data_stats\": {\n",
    "            \"total_reviews\": len(df),\n",
    "            \"date_range\": {\n",
    "                \"min\": df[\"ts\"].min().isoformat(),\n",
    "                \"max\": df[\"ts\"].max().isoformat()\n",
    "            },\n",
    "            \"unique_users\": df[\"user_id\"].nunique(),\n",
    "            \"unique_items\": df[\"item_id\"].nunique(),\n",
    "        },\n",
    "        \"preprocessing_steps\": [\n",
    "            \"1. Load JSON/JSONL reviews\",\n",
    "            \"2. Filter non-ASCII text (>=90% ASCII chars)\",\n",
    "            \"3. Drop rows missing user_id, item_id, or timestamp\",\n",
    "            \"4. Compute VADER sentiment + star-based sentiment\",\n",
    "            \"5. Create hybrid sentiment (alpha*text + (1-alpha)*stars)\",\n",
    "            \"6. Aggregate to monthly user-level trajectories\",\n",
    "            \"7. Compute drift metrics (slope, delta, TV, flip_rate)\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(run_dir, \"manifest.json\"), \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbbed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sanity_checks(df: pd.DataFrame, traj: pd.DataFrame):\n",
    "    issues = []\n",
    "    \n",
    "    # Check for unexpected NaN patterns\n",
    "    if df[\"sent_hybrid\"].isna().sum() > 0.1 * len(df):\n",
    "        issues.append(\"WARNING: >10% NaN in hybrid sentiment\")\n",
    "    \n",
    "    # Check date range makes sense\n",
    "    date_span = (df[\"ts\"].max() - df[\"ts\"].min()).days\n",
    "    if date_span < 30:\n",
    "        issues.append(\"WARNING: Data spans less than 30 days\")\n",
    "    \n",
    "    # Check drift metrics in reasonable range\n",
    "    if traj[\"drift_slope\"].abs().max() > 1.0:\n",
    "        issues.append(\"WARNING: Extreme drift_slope detected (>1.0 per month)\")\n",
    "    \n",
    "    # Check for duplicate user-month combinations\n",
    "    dupes = df.groupby([\"user_id\", \"month\"]).size().max()\n",
    "    if dupes > df.groupby([\"user_id\", \"month\"]).size().median() * 10:\n",
    "        issues.append(f\"WARNING: Some user-months have {dupes} reviews (outlier)\")\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70d39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 19:47:28,288 - INFO - [run] Starting baseline pipeline: baseline_v1_20251110_183220\n",
      "2025-11-10 19:47:28,358 - WARNING - WARNING: Extreme drift_slope detected (>1.0 per month)\n",
      "2025-11-10 19:47:28,360 - WARNING - WARNING: Some user-months have 20 reviews (outlier)\n",
      "2025-11-10 19:47:28,373 - INFO - [splits] Train: 1859, Val: 398, Test: 400\n",
      "2025-11-10 19:47:28,386 - INFO - [baseline] Mean drift slope: 0.0005\n",
      "2025-11-10 19:47:31,873 - INFO - [plots] Saved diagnostic plots to runs\\baseline_v1_20251110_183220\\figs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[plots] Saved diagnostic plots to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFIG_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# NEW: Reproducibility manifest\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m create_reproducibility_manifest(RUN_DIR, CONFIG, df_clean)\n\u001b[0;32m     42\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[done] Baseline run complete. Check \u001b[39m\u001b[38;5;132;01m{RUN_DIR}\u001b[39;00m\u001b[38;5;124m for all outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mcreate_reproducibility_manifest\u001b[1;34m(run_dir, config, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_reproducibility_manifest\u001b[39m(run_dir: \u001b[38;5;28mstr\u001b[39m, config: \u001b[38;5;28mdict\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Document everything needed to reproduce the run\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     manifest \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_info\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(run_dir),\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: dt\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[1;32m----> 7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_version\u001b[39m\u001b[38;5;124m\"\u001b[39m: sys\u001b[38;5;241m.\u001b[39mversion,\n\u001b[0;32m      8\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      9\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m: pd\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     10\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39m__version__,\n\u001b[0;32m     11\u001b[0m                 \u001b[38;5;66;03m# Add other packages\u001b[39;00m\n\u001b[0;32m     12\u001b[0m             }\n\u001b[0;32m     13\u001b[0m         },\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: config,\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     16\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df),\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_range\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m     18\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[0;32m     19\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[0;32m     20\u001b[0m             },\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_users\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(),\n\u001b[0;32m     22\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_items\u001b[39m\u001b[38;5;124m\"\u001b[39m: df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique(),\n\u001b[0;32m     23\u001b[0m         },\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessing_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Load JSON/JSONL reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2. Filter non-ASCII text (>=90\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m ASCII chars)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3. Drop rows missing user_id, item_id, or timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4. Compute VADER sentiment + star-based sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5. Create hybrid sentiment (alpha*text + (1-alpha)*stars)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6. Aggregate to monthly user-level trajectories\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7. Compute drift metrics (slope, delta, TV, flip_rate)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m         ]\n\u001b[0;32m     33\u001b[0m     }\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanifest.json\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     36\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(manifest, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    logger = setup_logging(RUN_DIR)\n",
    "    logger.info(f\"[run] Starting baseline pipeline: {run_id}\")\n",
    "    \n",
    "    issues = run_sanity_checks(df_clean, traj)\n",
    "    for issue in issues:\n",
    "        logger.warning(issue)\n",
    "\n",
    "    splits = create_data_splits(traj, monthly, strategy=\"temporal\")\n",
    "    logger.info(f\"[splits] Train: {len(splits['train'])}, Val: {len(splits['val'])}, Test: {len(splits['test'])}\")\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, \"splits.json\"), \"w\") as f:\n",
    "        json.dump({k: list(v) for k, v in splits.items()}, f, indent=2)\n",
    "    \n",
    "    baselines = compute_baselines(traj, splits)\n",
    "    logger.info(f\"[baseline] Mean drift slope: {baselines['mean_drift_slope']:.4f}\")\n",
    "    \n",
    "    eval_metrics = evaluate_drift_metrics(traj, splits, baselines)\n",
    "    metrics.update(eval_metrics)\n",
    "    \n",
    "    stat_results = statistical_tests(traj)\n",
    "    with open(os.path.join(RUN_DIR, \"statistical_tests.json\"), \"w\") as f:\n",
    "        json.dump(stat_results, f, indent=2)\n",
    "    \n",
    "    create_diagnostic_plots(traj, monthly, FIG_DIR)\n",
    "    logger.info(f\"[plots] Saved diagnostic plots to {FIG_DIR}\")\n",
    "    \n",
    "    create_reproducibility_manifest(RUN_DIR, CONFIG, df_clean)\n",
    "    \n",
    "    logger.info(\"[done] Baseline run complete. Check {RUN_DIR} for all outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e7dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
